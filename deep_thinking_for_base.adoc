=== 深度思考

==== 贝叶斯理论

[IMPORTANT]
.第一章的难点主要集中在如何理解下面几句话：
====
----
当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计 -- <1>
----
----
从贝叶斯的角度来看，正则化对应于模型的先验概率，可以假设复杂的模型具有较小的先验概率，简单的模型有较大的先验概率 -- <2>
----
====

<1> 当模型是条件概率分布时，例如逻辑回归，此时的参数估计常用最大似然估计，而很容易想到，损失函数应该取似然函数的负数。 +

\begin{align}
\theta_{ML} & = \underset{\theta}{argmax} \prod_{i=1}^{m} p( y^{i} \mid x^{i}; \theta) \hspace{4.2cm}(1) \\
L(\theta) & = \underset{\theta}{arg max}\sum_{i=1}^{m}log \hspace{0.1cm}p(y^{i} \mid x^{i}; \theta) \hspace{3.5cm}(2) \\
loss(\theta) & = \underset{\theta}{arg min}(-\sum_{i=1}^{m}log \hspace{0.1cm}p(y^{i} \mid x^{i}; \theta)) + L_{n}(\theta) \hspace{1cm}(3)
\end{align}

上面公式(1)为最大似然估计，公式(2)为对数似然函数，公式(3)是损失函数。 +

再来看下贝叶斯参数估计策略，贝叶斯方法的参数估计，就是通过最大化后验概率来估计模型的参数的。假定模型参数为\(w\),数据集为\(D\)，则： +

[stem]
++++
w = \underset{w}{arg max}p(w \mid D) = \underset{w}{argmax} \hspace{0.1cm} \frac{p(D \mid w) \cdot p(w)}{p(D)} \propto p(D \mid w) \cdot p(w)
++++

假定，样本独立不相关且模型参数独立不相关， 则 +

\begin{align}
p(w)p(D \mid w) & =  \prod_{i=1}^{m}p(D_{i} \mid w)\prod_{j=1}^{n}p(w_{j}) \\
& \propto\sum_{i=1}^{m}p(D_{i} \mid w) + \sum_{j=1}^{n}p(w_{j}) \hspace{1cm}(4)
\end{align}

对比公式(4)和公式(3)，公式(3)为损失函数，因此越小越好；公式(4)为后验概率，因此越大越好，将公式(4)取负号，其实就可以对应到损失函数，再看看描述，
模型是条件概率满足，损失函数为对数损失满足，如果模型复杂度由模型的先验概率表示，这句话可以对照公式(4)，其中先验概率就是\(p(w)\)累加项。那么这种情况下，
结构风险最小化(就是加了正则化项的最大似然估计)就等价于最大后验概率(就是似然函数加先验概率)，没毛病，刚好对上。 +

**第二个问题**：可以直接使用上一个题的结论，既然要比较，可以将最大后验概率取负，让两种思考方式都变成最小化问题，那么先验概率取负号对应模型的复杂度，
也就意味着复杂模型(值大)具有较小的先验概率(取负后值大)，反之亦然，证毕。 +
