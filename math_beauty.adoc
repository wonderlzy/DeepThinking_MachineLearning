== 数学之美

=== 数学中的空间

==== 空间关系

研究数学的学者对各种空间完全可以驾轻就熟，但是对于计算机专业又是刚开始搞机器学习的同学来说，未必能很好的掌握。 +
比如：什么是赋范线性空间、内积空间，度量空间，希尔伯特空间 ？ +

现代数学的一个特点就是以集合为研究对象，这样的好处就是可以将很多不同问题的本质抽象出来，变成同一个问题，当然这样的坏处就是描述起来比较抽象，很多人就难以理解了。 +

既然是研究集合，每个人感兴趣的角度不同，研究的方向也就不同。为了能有效地研究集合，必须给集合赋予一些“结构”（从一些具体问题抽象出来的结构）。
从数学的本质来看，最基本的集合有两类：线性空间（有线性结构的集合）、度量空间（有度量结构的集合）。 +

对线性空间而言，主要研究集合的描述，直观地说就是如何清楚地告诉地别人这个集合是什么样子。为了描述清楚，就引入了基（相当于三维空间中的坐标系）的概念，
所以对于一个线性空间来说，只要知道其基即可，集合中的元素只要知道其在给定基下的坐标即可。但线性空间中的元素没有“长度”（相当于三维空间中线段的长度），
为了量化线性空间中的元素，所以又在线性空间引入特殊的“长度”，即范数。赋予了范数的线性空间即称为赋犯线性空间。但赋范线性空间中两个元素之间没有角度的概念，
为了解决该问题，所以在线性空间中又引入了内积的概念。因为有度量，所以可以在度量空间、赋范线性空间以及内积空间中引入极限，但抽象空间中的极限与实数上的极限有一个很大的不同就是，
极限点可能不在原来给定的集合中，所以又引入了完备的概念，完备的内积空间就称为Hilbert空间。 +

这几个空间之间的关系是：线性空间与度量空间是两个不同的概念，没有交集。赋范线性空间就是赋予了范数的线性空间，也是度量空间（具有线性结构的度量空间）内积空间是赋范线性空间希尔伯特空间就是完备的内积空间。 +

==== 希尔伯特空间

做个类比,一般的3D矢量空间(我们最常见的)和Hilbert空间.在3D的矢量空间中, 基底是i,j,k. 维度是3(有限维). 这三个基本的基矢量是完备的(矢量空间中任何一个元素都可以用这3个基底展开,系数唯一), 正交的(不同的基底做点积为0.).  +

矢量空间中的任意两个元素之间可以定义算符F, 也就是操作. 我们常常对保持元素A长度(自己和自己点积,A*A)不变的操作感兴趣, 这样的操作形象上讲是转动, 抽象些讲是满足\(F^{2}=1\)的操作,或者叫变换. +

好了, 再看看Hilbert空间, 基底一般是函数,常见的是含有各种频率的平面波函数,一种频率对应一个基底 维度是无穷.这些基底, 即平面波函数是完备的(Hilbert空间中的任何元素都可以用平面波函数展开, 其实就是指傅里叶变换),
正交(平面波函数做"点积"为delta函数).  +

Hilbert空间中任意两元素也可以定义算符G, 也就是操作. 我们常常对保持元素"长度"(自己和自己"点积")不变的操作感兴趣. 由于Hilbert空间是复数域上的,常见的3D矢量空间是实数域上的, 所以\(G^{2}=1\) 的G 和F 是不同的, 虽然表达式相同. +

建立Hilbert空间的目的是为量子力学中的计算提供强有力的数学基础, 也方便了抽象出其中更本质的运算.包括之后进行的关于对称性的讨论, 都是定义在Hilbert空间上的.注意上面的论述中并没有涉及到矩阵.
因为矩阵其实只是抽象定义的一种表现, 或者说是抽象的定义的一种表示(representation)而已. 这是群论的思想.所有这些后面发展起来的表示理论, 在Hilbert空间中表示后,(尤其是算符的表示)显得非常重要.

=== 似然函数

1、似然与概率的区别 +

在英语语境里，likelihood 和 probability 的日常使用是可以互换的，都表示对机会 (chance) 的同义替代。但在数学中，probability 这一指代是有严格的定义的，即符合柯尔莫果洛夫公理 (Kolmogorov axioms) 的一种数学对象（换句话说，不是所有的可以用0到1之间的数所表示的对象都能称为概率），
而 likelihood (function) 这一概念是由Fisher提出，他采用这个词，也是为了凸显他所要表述的数学对象既和 probability 有千丝万缕的联系，但又不完全一样的这一感觉。中文把它们一个翻译为概率一个翻译为似然也是独具匠心。 +

除此之外，统计学中的另一常见概念"置信（区间）"(confidence interval)中的置信度(confidence level)也不是概率。换句话说，"构建关于总体均值的95%的置信区间"里的"95%"不是概率意义下的0.95（即使它也是0到1之间的代表机会chance的一个度量）；
更常见的p-值(p-value)严格来说其本身是一个(恰好位于0到1之间的)统计量(即样本随机变量的函数)，所以p-值也不是概率。 +

一种方便区别是概率还是似然的方法是，根据定义，"谁谁谁的概率"中谁谁谁只能是概率空间中的事件，换句话说，我们只能说，事件(发生)的概率是多少多少(因为事件具有概率结构从而刻画随机性，所以才能谈概率)；而"谁谁谁的似然"中的谁谁谁只能是参数，
比如说，参数等于\(\theta\) 时的似然是多少。  +

2、似然与概率的联系 +

先看似然函数的定义，它是给定联合样本值\(x\)下关于(未知)参数\(\theta\)的函数：\(L(\theta | x) = f(x | \theta)\) +
这里的小是\(x\)指联合样本随机变量\(X\)取到的值，即\(X=x\)； +
这里的\(\theta\)是指未知参数，它属于参数空间； +
这里的\(f(x | \theta)\)是一个密度函数，特别地，它表示(给定)\(\theta\)下关于联合样本值\(x\)的联合密度函数。 +
所以从定义上，似然函数和密度函数是完全不同的两个数学对象：前者是关于\(\theta\)的函数，后者是关于\(x\)的函数。所以这里的等号\(=\) 理解为函数值形式的相等，而不是两个函数本身是同一函数(根据函数相等的定义，函数相等当且仅当定义域相等并且对应关系相等)。 +

说完两者的区别，再说两者的联系。 +

（1）如果\(X\)是离散的随机向量，那么其概率密度函数\(f(x | \theta) \)可改写为\(f(x | \theta) = P_{\theta}(X = x)\)，即代表了在参数\(\theta\)下随机向量\(X\)取到值\(x\)的可能性；并且，如果我们发现 +

[stem]
++++
L(\theta_{1} | x) = P_{\theta_{1}}(X = x) > P_{\theta_{2}}(X = x) = L(\theta_{2} | x)
++++

那么似然函数就反应出这样一个朴素推测：在参数\(\theta_{1}\)下随机向量\(X\)取到值\(x\)的可能性大于 在参数\(theta_{2}\)下随机向量\(X\)取到值\(x\)的可能性。换句话说，我们更有理由相信(相对于\(\theta_{2}\)来说) \(\theta_{1}\) +
更有可能是真实值。这里的可能性由概率来刻画。 +

（2）如果\(X\)是连续的随机向量，那么其密度函数\(f(x | \theta)\)本身（如果在连续的话）在\(x\)处的概率为0，为了方便考虑一维情况：给定一个充分小\(\epsilon > 0\)，那么随机变量\(X\)取值在\((x-\epsilon, x+\epsilon)\)区间内的概率即为 +

[stem]
++++
P_{\theta} (x-\epsilon < X < x+ \epsilon) = \int_{x-\epsilon}^{x+\epsilon} f(x | \theta) dx \approx 2\epsilon f(x|\theta) =2\epsilon L(\theta | x)
++++

并且两个未知参数的情况下做比就能约掉\(2\epsilon\)，所以和离散情况下的理解一致，只是此时似然所表达的那种可能性和概率\(f(x| \theta = 0)\)无关。 +

综上，概率(密度)表达给定\(\theta\)下样本随机向量\(X=x\)的可能性，而似然表达了给定样本\(X=x\)下参数\(theta_{1}\)(相对于另外的参数\(\theta_{2}\))为真实值的可能性。我们总是对随机变量的取值谈概率，而在非贝叶斯统计的角度下，参数是一个实数而非随机变量，所以我们一般不谈一个参数的概率。 +

最后我们再回到\(L(\theta | x) = f(x | \theta)\)这个表达。首先我们严格记号，竖线\(|\)表示条件概率或者条件分布，分号\(;\)表示把参数隔开。所以这个式子的严格书写方式是\(L(\theta | x) = f(x; \theta)\)因为\(\theta\)在右端只当作参数理解。 +
