<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.7.1">
<meta name="description" content="机器学习深度思考学习笔记。从最简单的机器学习基本概念，逐层剖析，挖掘机器学习最本质的问题，展示机器学习背后的数学之美">
<meta name="keywords" content="机器学习，统计学习，逻辑回归，决策树，支持向量机，条件随机场，聚类">
<meta name="author" content="xjtu-zhongyingLi">
<title>深度思考之机器学习系列</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | http://asciidoctor.org */
/* Uncomment @import statement below to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}
audio,canvas,video{display:inline-block}
audio:not([controls]){display:none;height:0}
script{display:none!important}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:transparent}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:none}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #ddddd8;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt,table tr:nth-of-type(even){background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
*:not(pre)>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background-color:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed;word-wrap:break-word}
*:not(pre)>code.nobreak{word-wrap:normal}
*:not(pre)>code.nowrap{white-space:nowrap}
pre,pre>code{line-height:1.45;color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;text-rendering:optimizeSpeed}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background-color:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #ddddd8}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #ddddd8;padding-bottom:8px}
#header .details{border-bottom:1px solid #ddddd8;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #ddddd8;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #efefed;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background-color:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #efefed;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #efefed;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background-color:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #efefed}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #ddddd8;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock pre:not(.highlight),.listingblock pre[class="highlight"],.listingblock pre[class^="highlight "],.listingblock pre.CodeRay,.listingblock pre.prettyprint{background:#f7f7f8}
.sidebarblock .literalblock pre,.sidebarblock .listingblock pre:not(.highlight),.sidebarblock .listingblock pre[class="highlight"],.sidebarblock .listingblock pre[class^="highlight "],.sidebarblock .listingblock pre.CodeRay,.sidebarblock .listingblock pre.prettyprint{background:#f2f1f1}
.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;padding:1em;font-size:.8125em}
.literalblock pre.nowrap,.literalblock pre[class].nowrap,.listingblock pre.nowrap,.listingblock pre[class].nowrap{overflow-x:auto;white-space:pre;word-wrap:normal}
@media screen and (min-width:768px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.literalblock pre[class],.listingblock pre,.listingblock pre[class]{font-size:1em}}
.literalblock.output pre{color:#f7f7f8;background-color:rgba(0,0,0,.9)}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:#999}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:#999}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
table.pyhltable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.pyhltable td{vertical-align:top;padding-top:0;padding-bottom:0;line-height:1.45}
table.pyhltable td.code{padding-left:.75em;padding-right:0}
pre.pygments .lineno,table.pyhltable td:not(.code){color:#999;padding-left:0;padding-right:.5em;border-right:1px solid #ddddd8}
pre.pygments .lineno{display:inline-block;margin-right:.25em}
table.pyhltable .linenodiv{background:none!important;padding-right:0!important}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock blockquote p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.5em;margin-right:.5ex;text-align:right}
.quoteblock .quoteblock{margin-left:0;margin-right:0;padding:.5em 0;border-left:3px solid rgba(0,0,0,.6)}
.quoteblock .quoteblock blockquote{padding:0 0 0 .75em}
.quoteblock .quoteblock blockquote::before{display:none}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.abstract blockquote,.quoteblock.abstract blockquote p{word-spacing:0;line-height:1.6}
.quoteblock.abstract blockquote::before,.quoteblock.abstract p::before{display:none}
table.tableblock{max-width:100%;border-collapse:separate}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot,table.frame-ends{border-width:1px 0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd){background:#f8f8f7}
table.stripes-none tr,table.stripes-odd tr:nth-of-type(even){background:none}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
td>div.verse{white-space:pre}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left,.imageblock[style*="float: left"]{margin:.25em .625em 1.25em 0}
.imageblock.right,.imageblock[style*="float: right"]{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background-color:#00fafa}
.black{color:#000}
.black-background{background-color:#000}
.blue{color:#0000bf}
.blue-background{background-color:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background-color:#fa00fa}
.gray{color:#606060}
.gray-background{background-color:#7d7d7d}
.green{color:#006000}
.green-background{background-color:#007d00}
.lime{color:#00bf00}
.lime-background{background-color:#00fa00}
.maroon{color:#600000}
.maroon-background{background-color:#7d0000}
.navy{color:#000060}
.navy-background{background-color:#00007d}
.olive{color:#606000}
.olive-background{background-color:#7d7d00}
.purple{color:#600060}
.purple-background{background-color:#7d007d}
.red{color:#bf0000}
.red-background{background-color:#fa0000}
.silver{color:#909090}
.silver-background{background-color:#bcbcbc}
.teal{color:#006060}
.teal-background{background-color:#007d7d}
.white{color:#bfbfbf}
.white-background{background-color:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background-color:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background-color:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background-color:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #ddddd8!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
/* Stylesheet for CodeRay to match GitHub theme | MIT License | http://foundation.zurb.com */
/*pre.CodeRay {background-color:#f7f7f8;}*/
.CodeRay .line-numbers{border-right:1px solid #d8d8d8;padding:0 0.5em 0 .25em}
.CodeRay span.line-numbers{display:inline-block;margin-right:.5em;color:rgba(0,0,0,.3)}
.CodeRay .line-numbers strong{color:rgba(0,0,0,.4)}
table.CodeRay{border-collapse:separate;border-spacing:0;margin-bottom:0;border:0;background:none}
table.CodeRay td{vertical-align: top;line-height:1.45}
table.CodeRay td.line-numbers{text-align:right}
table.CodeRay td.line-numbers>pre{padding:0;color:rgba(0,0,0,.3)}
table.CodeRay td.code{padding:0 0 0 .5em}
table.CodeRay td.code>pre{padding:0}
.CodeRay .debug{color:#fff !important;background:#000080 !important}
.CodeRay .annotation{color:#007}
.CodeRay .attribute-name{color:#000080}
.CodeRay .attribute-value{color:#700}
.CodeRay .binary{color:#509}
.CodeRay .comment{color:#998;font-style:italic}
.CodeRay .char{color:#04d}
.CodeRay .char .content{color:#04d}
.CodeRay .char .delimiter{color:#039}
.CodeRay .class{color:#458;font-weight:bold}
.CodeRay .complex{color:#a08}
.CodeRay .constant,.CodeRay .predefined-constant{color:#008080}
.CodeRay .color{color:#099}
.CodeRay .class-variable{color:#369}
.CodeRay .decorator{color:#b0b}
.CodeRay .definition{color:#099}
.CodeRay .delimiter{color:#000}
.CodeRay .doc{color:#970}
.CodeRay .doctype{color:#34b}
.CodeRay .doc-string{color:#d42}
.CodeRay .escape{color:#666}
.CodeRay .entity{color:#800}
.CodeRay .error{color:#808}
.CodeRay .exception{color:inherit}
.CodeRay .filename{color:#099}
.CodeRay .function{color:#900;font-weight:bold}
.CodeRay .global-variable{color:#008080}
.CodeRay .hex{color:#058}
.CodeRay .integer,.CodeRay .float{color:#099}
.CodeRay .include{color:#555}
.CodeRay .inline{color:#000}
.CodeRay .inline .inline{background:#ccc}
.CodeRay .inline .inline .inline{background:#bbb}
.CodeRay .inline .inline-delimiter{color:#d14}
.CodeRay .inline-delimiter{color:#d14}
.CodeRay .important{color:#555;font-weight:bold}
.CodeRay .interpreted{color:#b2b}
.CodeRay .instance-variable{color:#008080}
.CodeRay .label{color:#970}
.CodeRay .local-variable{color:#963}
.CodeRay .octal{color:#40e}
.CodeRay .predefined{color:#369}
.CodeRay .preprocessor{color:#579}
.CodeRay .pseudo-class{color:#555}
.CodeRay .directive{font-weight:bold}
.CodeRay .type{font-weight:bold}
.CodeRay .predefined-type{color:inherit}
.CodeRay .reserved,.CodeRay .keyword {color:#000;font-weight:bold}
.CodeRay .key{color:#808}
.CodeRay .key .delimiter{color:#606}
.CodeRay .key .char{color:#80f}
.CodeRay .value{color:#088}
.CodeRay .regexp .delimiter{color:#808}
.CodeRay .regexp .content{color:#808}
.CodeRay .regexp .modifier{color:#808}
.CodeRay .regexp .char{color:#d14}
.CodeRay .regexp .function{color:#404;font-weight:bold}
.CodeRay .string{color:#d20}
.CodeRay .string .string .string{background:#ffd0d0}
.CodeRay .string .content{color:#d14}
.CodeRay .string .char{color:#d14}
.CodeRay .string .delimiter{color:#d14}
.CodeRay .shell{color:#d14}
.CodeRay .shell .delimiter{color:#d14}
.CodeRay .symbol{color:#990073}
.CodeRay .symbol .content{color:#a60}
.CodeRay .symbol .delimiter{color:#630}
.CodeRay .tag{color:#008080}
.CodeRay .tag-special{color:#d70}
.CodeRay .variable{color:#036}
.CodeRay .insert{background:#afa}
.CodeRay .delete{background:#faa}
.CodeRay .change{color:#aaf;background:#007}
.CodeRay .head{color:#f8f;background:#505}
.CodeRay .insert .insert{color:#080}
.CodeRay .delete .delete{color:#800}
.CodeRay .change .change{color:#66f}
.CodeRay .head .head{color:#f4f}
</style>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>深度思考之机器学习系列</h1>
<div class="details">
<span id="author" class="author">xjtu-zhongyingLi</span><br>
<span id="revdate">2018-07-18</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">目录</div>
<ul class="sectlevel1">
<li><a href="#_机器学习基本概念">1. 机器学习基本概念</a>
<ul class="sectlevel2">
<li><a href="#_统计学习">1.1. 统计学习</a></li>
<li><a href="#_统计学习三要素">1.2. 统计学习三要素</a>
<ul class="sectlevel3">
<li><a href="#_模型">1.2.1. 模型</a></li>
<li><a href="#_策略">1.2.2. 策略</a>
<ul class="sectlevel4">
<li><a href="#_损失函数">1.2.2.1. 损失函数</a></li>
<li><a href="#_经验风险最小化">1.2.2.2. 经验风险最小化</a></li>
<li><a href="#_结构风险最小化">1.2.2.3. 结构风险最小化</a></li>
</ul>
</li>
<li><a href="#_算法">1.2.3. 算法</a></li>
</ul>
</li>
<li><a href="#_模型评估">1.3. 模型评估</a>
<ul class="sectlevel3">
<li><a href="#_正则化">1.3.1. 正则化</a></li>
<li><a href="#_交叉验证">1.3.2. 交叉验证</a></li>
<li><a href="#_泛化能力">1.3.3. 泛化能力</a></li>
<li><a href="#_生成模型和判别模型">1.3.4. 生成模型和判别模型</a>
<ul class="sectlevel4">
<li><a href="#_判别模型">1.3.4.1. 判别模型</a></li>
<li><a href="#_生成模型">1.3.4.2. 生成模型</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_深度思考">1.4. 深度思考</a>
<ul class="sectlevel3">
<li><a href="#_贝叶斯理论">1.4.1. 贝叶斯理论</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_线性回归与逻辑回归">2. 线性回归与逻辑回归</a>
<ul class="sectlevel2">
<li><a href="#_线性回归">2.1. 线性回归</a>
<ul class="sectlevel3">
<li><a href="#_概念">2.1.1. 概念</a></li>
<li><a href="#_梯度下降">2.1.2. 梯度下降</a></li>
<li><a href="#_梯度下降的局限性">2.1.3. 梯度下降的局限性</a></li>
<li><a href="#_深度思考_2">2.1.4. 深度思考</a></li>
</ul>
</li>
<li><a href="#_逻辑回归">2.2. 逻辑回归</a>
<ul class="sectlevel3">
<li><a href="#_揭开面纱">2.2.1. 揭开面纱</a></li>
<li><a href="#_sigmoid函数">2.2.2. Sigmoid函数</a></li>
<li><a href="#_参数更新">2.2.3. 参数更新</a></li>
<li><a href="#_深度思考_3">2.2.4. 深度思考</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_特征工程">3. 特征工程</a>
<ul class="sectlevel2">
<li><a href="#_引子">3.1. 引子</a></li>
<li><a href="#_特征工程_2">3.2. 特征工程</a>
<ul class="sectlevel3">
<li><a href="#_数据清洗">3.2.1. 数据清洗</a></li>
<li><a href="#_数据采样">3.2.2. 数据采样</a></li>
<li><a href="#_特征处理">3.2.3. 特征处理</a>
<ul class="sectlevel4">
<li><a href="#_数值型特征">3.2.3.1. 数值型特征</a></li>
<li><a href="#_类别型特征">3.2.3.2. 类别型特征</a></li>
<li><a href="#_时间型特征">3.2.3.3. 时间型特征</a></li>
<li><a href="#_文本型特征">3.2.3.4. 文本型特征</a></li>
<li><a href="#_统计特征">3.2.3.5. 统计特征</a></li>
<li><a href="#_组合特征">3.2.3.6. 组合特征</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#_支持向量机">4. 支持向量机</a></li>
<li><a href="#_主题模型">5. 主题模型</a>
<ul class="sectlevel2">
<li><a href="#_解决什么问题">5.1. 解决什么问题</a></li>
<li><a href="#_给问题建模">5.2. 给问题建模</a>
<ul class="sectlevel3">
<li><a href="#_建模">5.2.1. 建模</a></li>
<li><a href="#_第一次思考">5.2.2. 第一次思考</a></li>
<li><a href="#_第二次思考">5.2.3. 第二次思考</a>
<ul class="sectlevel4">
<li><a href="#_一个重大的发现">5.2.3.1. 一个重大的发现</a></li>
<li><a href="#_metropolis_hastings算法">5.2.3.2. Metropolis Hastings算法</a></li>
<li><a href="#_gibbs_sampling_算法">5.2.3.3. Gibbs Sampling 算法</a></li>
</ul>
</li>
<li><a href="#_第三次思考">5.2.4. 第三次思考</a></li>
</ul>
</li>
<li><a href="#_文本建模">5.3. 文本建模</a>
<ul class="sectlevel3">
<li><a href="#_文档是如何产生的">5.3.1. 文档是如何产生的</a></li>
<li><a href="#_plsa模型">5.3.2. PLSA模型</a></li>
<li><a href="#_lda模型">5.3.3. LDA模型</a>
<ul class="sectlevel4">
<li><a href="#_二项分布">5.3.3.1. 二项分布</a></li>
<li><a href="#_神奇的_gamma_函数">5.3.3.2. 神奇的 <code>Gamma</code> 函数</a></li>
<li><a href="#_beta_函数">5.3.3.3. <code>Beta</code> 函数</a></li>
<li><a href="#_第四次思考">5.3.3.4. 第四次思考</a></li>
<li><a href="#_dirichlet_分布">5.3.3.5. <code>Dirichlet</code> 分布</a></li>
<li><a href="#_lda_模型">5.3.3.6. <code>LDA</code> 模型</a></li>
<li><a href="#_模型训练和推演">5.3.3.7. 模型训练和推演</a></li>
</ul>
</li>
<li><a href="#_变分em算法求解plsa模型">5.3.4. 变分EM算法求解pLSA模型</a>
<ul class="sectlevel4">
<li><a href="#_em算法">5.3.4.1. EM算法</a></li>
<li><a href="#_求解plsa算法">5.3.4.2. 求解pLSA算法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_终篇">5.4. 终篇</a></li>
</ul>
</li>
<li><a href="#_最大熵理论和多分类器">6. 最大熵理论和多分类器</a></li>
<li><a href="#_em_算法">7. <code>EM</code> 算法</a></li>
<li><a href="#_决策树和集成学习">8. 决策树和集成学习</a></li>
<li><a href="#_推荐系统">9. 推荐系统</a></li>
<li><a href="#_聚类和近邻算法">10. 聚类和近邻算法</a></li>
<li><a href="#_神经网络">11. 神经网络</a></li>
<li><a href="#_隐马尔可夫模型">12. 隐马尔可夫模型</a></li>
<li><a href="#_条件随机场">13. 条件随机场</a></li>
<li><a href="#_深度学习概论">14. 深度学习概论</a></li>
<li><a href="#_深度学习中的优化算法">15. 深度学习中的优化算法</a></li>
<li><a href="#_深度学习中的正则化">16. 深度学习中的正则化</a></li>
<li><a href="#_深度神经网络">17. 深度神经网络</a></li>
<li><a href="#_卷积神经网络">18. 卷积神经网络</a></li>
<li><a href="#_循环神经网络">19. 循环神经网络</a></li>
<li><a href="#_神经网络番外篇">20. 神经网络番外篇</a></li>
<li><a href="#_生成对抗网络">21. 生成对抗网络</a></li>
<li><a href="#_迁移学习">22. 迁移学习</a></li>
<li><a href="#_数学之美">23. 数学之美</a></li>
<li><a href="#_深度学习框架">24. 深度学习框架</a></li>
<li><a href="#_走进互联网">25. 走进互联网</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="quoteblock">
<blockquote>
数学是人类智慧的结晶，统计学习是数学领域最璀璨的明珠，算法可以让这个明珠照亮整个世界!
</blockquote>
<div class="attribution">
&#8212; 李中英<br>
<cite>世界知名互联网公司高级算法研究猿👍👍👍</cite>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_机器学习基本概念">1. 机器学习基本概念</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_统计学习">1.1. 统计学习</h3>
<div class="paragraph">
<p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，也成为统计机器学习。
统计学习的对象是数据，它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去，
统计学习关于数据的基本假设是：同类数据具有一定的统计规律性.<br></p>
</div>
<div class="paragraph">
<p>统计学习的目的是对数据进行预测与分析，是通过构建概率统计模型实现的，统计学习总的目的就是考虑学习什么样的模型和如何学习模型，
以使模型能对数据进行准确的预测和分析，同时考虑尽可能的提高学习效率.<br></p>
</div>
<div class="paragraph">
<p>统计学习的方法包括：监督学习、非监督学习、半监督学习和强化学习，我们重点讨论监督学习.<br></p>
</div>
</div>
<div class="sect2">
<h3 id="_统计学习三要素">1.2. 统计学习三要素</h3>
<div class="sect3">
<h4 id="_模型">1.2.1. 模型</h4>
<div class="paragraph">
<p>在监督学习中，模型就是指要学习的条件概率分布或决策函数。假设空间中的模型一般有无穷多个，假设空间可定义为：<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
F=\left \{ f|Y=f(X) \right \}
\end{equation}</p>
</div>
<div class="paragraph">
<p>假设空间通常是由参数向量决定的函数簇，其中参数向量取值于n维欧氏空间，称为参数空间。</p>
</div>
<div class="paragraph">
<p>\begin{equation}
F=\left \{ f|Y=f_{\theta }(X),\theta \in R^{n} \right \}
\end{equation}</p>
</div>
<div class="paragraph">
<p>假设空间也可以定义为条件概率的集合:</p>
</div>
<div class="paragraph">
<p>\begin{equation}
F=\left \{ P|P_{\theta }(Y|X),\theta \in R^{n} \right \}
\end{equation}</p>
</div>
</div>
<div class="sect3">
<h4 id="_策略">1.2.2. 策略</h4>
<div class="paragraph">
<p>有了模型的假设空间，统计学习接着要考虑的是按照什么样的准则学习或选择最优的模型，这就是策略。<br></p>
</div>
<div class="sect4">
<h5 id="_损失函数">1.2.2.1. 损失函数</h5>
<div class="paragraph">
<p>策略用来解决最优模型的选择问题，那么如何评价模型优劣，这就是损失函数或代价函数，下面是一些常见的损失函数：<br>
（1）0-1损失<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
L(Y|f(X))=\left\{\begin{matrix}
1,Y\neq f(X) &amp; \\
0,Y=f(X)&amp;
\end{matrix}\right.
\end{equation}</p>
</div>
<div class="paragraph">
<p>（2）平方损失<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
L(Y|f(X))=(Y-f(X))^{2}
\end{equation}</p>
</div>
<div class="paragraph">
<p>（3）绝对损失<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
L(Y|f(X))=\left |Y-f(X) \right |
\end{equation}</p>
</div>
<div class="paragraph">
<p>（4）对数损失<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
L(Y|f(X))=-log(P(Y|X)
\end{equation}</p>
</div>
<div class="paragraph">
<p>损失函数越小，模型就越好，理论上的最优模型应该是损失函数的期望值最小，而理论模型是关于联合分布下的平均损失，称为期望损失或风险损失，然而现实的问题是联合分布是未知的，如果已知也就不需要学习了。<br></p>
</div>
<div class="paragraph">
<p>所以机器学习采用的方法时是通过用<strong>训练数据集上的平均损失近似期望损失</strong>，训练集上的风险我们称为经验风险，根据大数定理，当训练样本数量趋近于无穷时，经验风险趋近于期望风险。<br></p>
</div>
<div class="paragraph">
<p>但是现实中的训练样本数量是有限的，甚至很小，所以直接使用经验风险估计期望风险常常不理想，需要对经验风险进行一定的矫正，这就关系到监督学习的两个基本策略：<strong>经验风险最小化和结构风险最小化</strong>。<br></p>
</div>
</div>
<div class="sect4">
<h5 id="_经验风险最小化">1.2.2.2. 经验风险最小化</h5>
<div class="paragraph">
<p>经验风险最小化的策略认为：经验风险最小化的模型是最优模型，根据这一策略，
按照经验风险最小化策略求最优模型就是求解最优化问题：<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
min\frac{1}{m}\sum_{i=1}^{m}L(y_{i},f(x_{i}))
\end{equation}</p>
</div>
<div class="paragraph">
<p>极大似然估计就是经验风险最小化的典型例子，当模型是条件概率，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。<br></p>
</div>
<div class="paragraph">
<p>但是，当样本量较少时，经验风险最小化学习的效果未必很好，会产生过拟合的问题，结构风险最小化是为了防止过拟合而提出的策略。<br></p>
</div>
</div>
<div class="sect4">
<h5 id="_结构风险最小化">1.2.2.3. 结构风险最小化</h5>
<div class="paragraph">
<p>结构风险最小化等价于正则化，结构风险在经验风险上加上表示模型复杂度的正则化项或惩罚项，结构风险求解的最优化问题是：<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
min\frac{1}{m}\sum_{i=1}^{m}L(y_{i},f(x_{i}))+\lambda J(f)
\end{equation}</p>
</div>
<div class="paragraph">
<p>其中 \(J(f)\) 为模型复杂度，\(\lambda \geqslant 0\)是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小，结构风险小的模型往往对未知的测试数据和已知的训练数据都有较好的预测.<br></p>
</div>
<div class="paragraph">
<p>比如，贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。<strong>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计</strong>。<br></p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">经验风险最小化深度理解</div>
<div class="paragraph">
<p>模型是条件概率分布，优化目标可以表示为概率连乘的形势，如果损失函数是对数损失，即可以写成连加的形势，由于最大后验概率可以写成似然函数和先验概率分布的乘积，对数损失后，先验概率就变成似然函数连加后的一项，对比上面公式，先验概率就刚好等价于模型复杂度，因此这种情况下，结构风险最小化就等价于最大后验概率估计。<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>我们将会在一个独立的小节阐述如何用贝叶斯理论理解本章的概念</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_算法">1.2.3. 算法</h4>
<div class="paragraph">
<p>从假设空间选择最优模型后，需要考虑用什么计算方法求解最优模型，如果最优化问题有显式的解析解，这个最优化问题就比较简单，但通常解析解不存在，这就需要用数值计算的方法求解，如何保证找到全局最优解，并使求解的过程高效，是算法最核心的问题。<br></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_模型评估">1.3. 模型评估</h3>
<div class="paragraph">
<p>统计学习的目的是使学到的模型不仅对已知数据且对未知数据都有很好的预测能力，一般采用训练误差和测试误差作为评价模型的标准，而最终决定模型是否真的不错，是由模型在未知数据上的预测能力决定的。<br></p>
</div>
<div class="paragraph">
<p>通常将学习方法对未知数据的预测能力称为泛化能力。因此选择模型时一定是选择泛化能力强的模型，而这个指标很难量化，通常在模型选择时参考奥卡姆剃刀原理，即当两个模型在测试集上具有相近的误差时，倾向于选择更简单的模型。<br></p>
</div>
<div class="paragraph">
<p>换句话说，越复杂的模型，其过拟合的风险也就越高，一味追求对训练数据的预测能力，会导致模型的复杂度高于真是模型的复杂度，这种现象就是<strong>过拟合</strong>。<br></p>
</div>
<div class="paragraph">
<p>避免模型过拟合的方法有很多，我们重点介绍常用的两种方法：<strong>正则化和交叉验证</strong>。</p>
</div>
<div class="sect3">
<h4 id="_正则化">1.3.1. 正则化</h4>
<div class="paragraph">
<p>模型选择的典型方法是正则化，正则化是结构最小化策略的实现，是在经验风险上加上一个正则化项或惩罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大，正则化一般的形势：<br></p>
</div>
<div class="paragraph">
<p>\begin{equation}
min\frac{1}{m}\sum_{i=1}^{m}L(y_{i},f(x_{i}))+\lambda J(f)
\end{equation}</p>
</div>
<div class="paragraph">
<p>其中，第一项是经验风险，第二项是正则化项，正则化项可以取不同的形势，常见的有L1和L2正则化.<br></p>
</div>
<div class="paragraph">
<p>正则化的作用是选择经验风险和模型复杂度同时较小的模型，正则化符合奥卡姆剃刀原理：在所有可能选择的模型中，能够很好的解释已知数据并且十分简单才是最好的模型。<strong>从贝叶斯的角度来看，正则化对应于模型的先验概率，可以假设复杂的模型具有较小的先验概率，简单的模型有较大的先验概率</strong>。<br></p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">正则化项深入理解</div>
<div class="paragraph">
<p>如果正则化项等价于模型的复杂度，那么复杂模型的正则化项应该较大，如果正则化想对应于模型的先验概率，那么复杂模型应该具有较大的先验概率才对？ 其实，最大后验概率是一个\(max\)问题，而最优化问题是一个\(min\)问题，在最大后验概率转换为最优化问题时，需要给优化项取负值。</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_交叉验证">1.3.2. 交叉验证</h4>
<div class="paragraph">
<p>经验告诉我们，交叉验证非常非常重要，任何模型都会多少有一些超参数需要调（即调参），不同的超参数对应了不同的模型，如果选择超参数和对应的模型呢？让不同超参数对应的模型都在同一份验证集上评估，选择性能最优的模型。<br></p>
</div>
<div class="paragraph">
<p>理想条件下，当样本数据充足时，一般将数据分成：训练集、验证集和测试集，即模型的学习有完全独立的验证数据，训练集负责模型的训练，验证集负责模型选择，而测试集负责模型最终的评估。<br></p>
</div>
<div class="paragraph">
<p>实际情况下，样本的数据量往往较少，此时常用的交叉验证有：简单交叉验证-将数据分成7:3的训练集和测试集，测试集负责验证和模型选择；K折交叉验证-将数据随机分成K份，选取其中一份作为测试集，其余K-1份训练，将这一过程进行K次选择重复进行，最后选出K次评估中平均测试误差最小的模型；留一法-当数据严重缺乏时使用，实际应用很少。<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_泛化能力">1.3.3. 泛化能力</h4>
<div class="paragraph">
<p>泛化能力是模型最本质的要求，也机器学习中最核心的概念。经验风险最小化的角度考虑，训练误差小的模型，其泛化误差也会小，应用Hoeffding不等式证明泛化误差的上界。<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_生成模型和判别模型">1.3.4. 生成模型和判别模型</h4>
<div class="sect4">
<h5 id="_判别模型">1.3.4.1. 判别模型</h5>
<div class="paragraph">
<p>以二分类问题为例，在解空间中寻找一条直线把两种类别的样本分开，对于新的样本只需判断在直线的哪一侧即可，这种直接对问题求解的的方法称为判别模型。<br></p>
</div>
</div>
<div class="sect4">
<h5 id="_生成模型">1.3.4.2. 生成模型</h5>
<div class="paragraph">
<p>生成模型会首先对两类样本分别进行建模，用新的样本去分别匹配两个模型，匹配度高的作为新样本的类别。</p>
</div>
<div class="paragraph">
<p>形式化地说，判别模型是直接对进行建模或者直接学习输入空间到输出空间的映射关系，而生成模型是对条件概率和先验概率进行建模，然后按照贝叶斯公式求出后验概率。使得后验概率最大的类别就是新样本的预测值。<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(y|x) = \frac{p(y)\cdot p(x|y)}{p(x)} \\
\underset{y}{argmax\, p(y|x)} = \underset{y}{argmax\, p(x|y)\cdot p(y)}
\end{align}</p>
</div>
<div class="exampleblock">
<div class="title">示例 1. 贝叶斯学派下的生成模型是如何对未知样本进行预测</div>
<div class="content">
<div class="paragraph">
<p>假设仍然是个二分类问题，问题是预测一个人是男人还是女人，为了简单起见，假设特征只有一个：是否有胡子。</p>
</div>
<div class="listingblock">
<div class="content">
<pre>思路肯定是分别求解新样本是男人和女人的概率，取概率最大的类别作为预测结果，假定新样本为"有胡子"：</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>p(y=male|x=beard) = p(x=beard|y=male)✖️p(y=male)<i class="conum" data-value="1"></i><b>(1)</b>

p(y=female|x=beard) = p(x=beard|y=female)✖️p(y=female) <i class="conum" data-value="2"></i><b>(2)</b></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>关键在于如何求解上面两个概率值，其实上面公式的概率值全部都是统计值，也就是训练样本中，根据条件统计出来的概率。比如第一个公式，是男人的概率就是样本中男人的占比，而条件概率（似然函数）就是男人中有胡子的概率。</pre>
</div>
</div>
<div class="paragraph">
<p><em>你可能会问特征参数去哪了？求特征参数的方法是判别模型，而生成模型不需要!</em></p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_深度思考">1.4. 深度思考</h3>
<div class="sect3">
<h4 id="_贝叶斯理论">1.4.1. 贝叶斯理论</h4>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">第一章的难点主要集中在如何理解下面几句话：</div>
<div class="listingblock">
<div class="content">
<pre>当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计 <i class="conum" data-value="1"></i><b>(1)</b></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>从贝叶斯的角度来看，正则化对应于模型的先验概率，可以假设复杂的模型具有较小的先验概率，简单的模型有较大的先验概率 <i class="conum" data-value="2"></i><b>(2)</b></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>当模型是条件概率分布时，例如逻辑回归，此时的参数估计常用最大似然估计，而很容易想到，损失函数应该取似然函数的负数。<br></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>\begin{align}
\theta_{ML} &amp; = \underset{\theta}{argmax} \prod_{i=1}^{m} p( y^{i} \mid x^{i}; \theta) \hspace{4.2cm}(1) \\
L(\theta) &amp; = \underset{\theta}{arg max}\sum_{i=1}^{m}log \hspace{0.1cm}p(y^{i} \mid x^{i}; \theta) \hspace{3.5cm}(2) \\
loss(\theta) &amp; = \underset{\theta}{arg min}(-\sum_{i=1}^{m}log \hspace{0.1cm}p(y^{i} \mid x^{i}; \theta)) + L_{n}(\theta) \hspace{1cm}(3)
\end{align}</p>
</div>
<div class="paragraph">
<p>上面公式(1)为最大似然估计，公式(2)为对数似然函数，公式(3)是损失函数。<br></p>
</div>
<div class="paragraph">
<p>再来看下贝叶斯参数估计策略，贝叶斯方法的参数估计，就是通过最大化后验概率来估计模型的参数的。假定模型参数为\(w\),数据集为\(D\)，则：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[w = \underset{w}{arg max}p(w \mid D) = \underset{w}{argmax} \hspace{0.1cm} \frac{p(D \mid w) \cdot p(w)}{p(D)} \propto p(D \mid w) \cdot p(w)\]
</div>
</div>
<div class="paragraph">
<p>假定，样本独立不相关且模型参数独立不相关， 则<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(w)p(D \mid w) &amp; =  \prod_{i=1}^{m}p(D_{i} \mid w)\prod_{j=1}^{n}p(w_{j}) \\
&amp; \propto\sum_{i=1}^{m}p(D_{i} \mid w) + \sum_{j=1}^{n}p(w_{j}) \hspace{1cm}(4)
\end{align}</p>
</div>
<div class="paragraph">
<p>对比公式(4)和公式(3)，公式(3)为损失函数，因此越小越好；公式(4)为后验概率，因此越大越好，将公式(4)取负号，其实就可以对应到损失函数，再看看描述，
模型是条件概率满足，损失函数为对数损失满足，如果模型复杂度由模型的先验概率表示，这句话可以对照公式(4)，其中先验概率就是\(p(w)\)累加项。那么这种情况下，
结构风险最小化(就是加了正则化项的最大似然估计)就等价于最大后验概率(就是似然函数加先验概率)，没毛病，刚好对上。<br></p>
</div>
<div class="paragraph">
<p><strong>第二个问题</strong>：可以直接使用上一个题的结论，既然要比较，可以将最大后验概率取负，让两种思考方式都变成最小化问题，那么先验概率取负号对应模型的复杂度，
也就意味着复杂模型(值大)具有较小的先验概率(取负后值大)，反之亦然，证毕。<br></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_线性回归与逻辑回归">2. 线性回归与逻辑回归</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_线性回归">2.1. 线性回归</h3>
<div class="sect3">
<h4 id="_概念">2.1.1. 概念</h4>
<div class="paragraph">
<p>线形回归主要研究一个变量(\(y\))关于另一些变量(\(X\))的具体依赖关系的。例如，房价问题，假设我们只考虑房屋的面积和卧室的个数，问题就可以具体解释为：我们希望得到这样一个线形模型，使得给它一个房子的面积和卧室个数，它就可以准确的评估出房价。<br></p>
</div>
<div class="paragraph">
<p>假设房价为\(y\)，房屋面积为\(x_{1}\)，卧室个数为\(x_{2}\)，那么线形模型就可以表示为：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[y=\theta _{0}x_{0}+\theta _{1}x_{1}+\theta _{2}x_{2}=\sum_{i=0}^{i=2}\theta _{i}x_{i}\hspace{2cm}(2.1)\]
</div>
</div>
<div class="paragraph">
<p>其中\(\theta \)为要学习的参数，这里\(x_{0}=1\)，对于一般问题，公式通常写成：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[h_{\theta }(x) = \sum_{i=0}^{m}\theta ^{i}x^{i}=\Theta ^{T}X\hspace{4cm} (2.2)\]
</div>
</div>
<div class="paragraph">
<p>我们用\(x^{i}\)表示第\(i\)个特征，\( y^{i} \)表示\(x^{i}\)对应的房价，从而定义损失函数\( J(\theta)\)，一个很自然的想法就是用所有样本预测值和真实值的平方差和作为损失函数：</p>
</div>
<div class="stemblock">
<div class="content">
\[J(\theta )=\frac{1}{2}\sum_{i=0}^{m}(h_{\theta}(x^{i})-y^{i})^{2}\hspace{3.5cm}(2.3)\]
</div>
</div>
<div class="paragraph">
<p>我们的目标就是找出使损失函数最小的参数值，就得到了拟合训练集的最佳参数，至于为什么使用该函数会取得很好的效果，后面会有解释，这种通过最小化误差的平方和来求解最佳参数的方法又称为 <code>最小二乘法或最小平方误差</code>。<br></p>
</div>
<div class="paragraph">
<p>求一个函数的最小值，最简单的方法是对函数求导，令导数为0，直接解出参数，这种方法叫 <code>解正规方程组</code>，方法简单直接，却有很多限制，比如未知数（即参数）个数大于方程组个数（即训练样本）时，无法求解，该方法我们将在后面单独讲述。<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_梯度下降">2.1.2. 梯度下降</h4>
<div class="paragraph">
<p>这里我们重点介绍 <code>梯度下降（Gradient Descent）</code> 来求参数，即通过不断调整参数的值，使得损失函数值不断减小，迭代收敛至满足损失误差允许的范围，一个直观的更新规则为：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} + \delta \hspace{6cm}(2.4)\]
</div>
</div>
<div class="paragraph">
<p>关键就在于每次迭代如何求解\(\delta\)，确定\(\delta\)是什么的过程不是一个数学上严格的推断（即没有标准答案），而是一种猜想（也正是因为没有标准答案所以更新参数的方法除了梯度下降，还有牛顿法、拟牛顿法等），我们要做的就是如何让猜想尽可能的合理。<br></p>
</div>
<div class="paragraph">
<p>首先介绍梯度的概念，我们中学学过的导数即梯度，反映的是 <code>参数往正向的变化量趋向于0时的其函数值变化量的极限</code>，比如梯度为5，表示参数往梯度方向增加一点点时，其函数值也增加一点点，但不一定是5；再比如梯度为-5，参数往梯度方向增加一点点的时候，其函数值会将少一点点。而我们的目的是让函数值不断减小，因此不管梯度是正还是负，我们只要将参数往梯度相反的方向增加一点点，函数值就会减小，则更新规则就可以变成:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} + \delta = \theta_{j} - \frac{\partial }{\partial \theta _{j}}J(\theta) \hspace{2.5cm}(2.5)\]
</div>
</div>
<div class="paragraph">
<p>此时的\(\delta\)不再是未知数，而是损失函数的负梯度。至此，我们仅确定了没步迭代更新的方向，而每次更新多少合适呢？这个值确实很难给出，不妨引入步长因子\(\alpha\)作为超参数，那么最终的更新规则就变为:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j}-\alpha \frac{\partial }{\partial \theta _{j}}J(\theta) \hspace{4.2cm}(2.6)\]
</div>
</div>
<div class="exampleblock">
<div class="title">示例 2. 关于梯度下降</div>
<div class="content">
<div class="paragraph">
<p>关于如何理解更新规则的最终形势，大家经常讲到的一个故事场景：想想一下你站在山腰（任意初始位置）上，目标是走到山谷（最小值），你将如何走才能尽快的到达山谷？沿着最陡峭的方向迈一大步！这个最陡峭的方向就是副梯度方向，而你迈得一大步就是步长，一直这样走下去，你会走到谷底。<br></p>
</div>
</div>
</div>
<div class="paragraph">
<p>言归正传，根据公式(2.6)发现：问题的关键就在于如何求解损失函数即公式(2.3)的梯度，下面给出求解过程（假定只有一个训练样本）：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\frac{\partial}{\partial \theta_{j}}J(\theta) &amp; = \frac{\partial}{\partial \theta_{j}} \frac{1}{2}(h_{\theta}(x)-y)^{2} \\
&amp; = (h_{\theta}(x)-y)\frac{\partial}{\partial \theta_{j}}h_{\theta}(x) \\
&amp; = x_{j}(h_{\theta}(x)-y) \hspace{3cm} (2.7)
\end{align}</p>
</div>
<div class="paragraph">
<p>将公式(2.7)带入公式(2.4)得到：</p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} - x_{j}(h_{\theta}(x)-y) \hspace{3.5cm} (2.8)\]
</div>
</div>
<div class="paragraph">
<p>公式(2.7)是针对只有一个训练样本的情况，考虑到所有 <code>m</code> 个样本时，更新规则就变为：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} - x_{j}^{i}\sum_{i=0}^{m}(h_{\theta}(x^{i})-y^{i}) \hspace{2.5cm} (2.9)\]
</div>
</div>
<div class="paragraph">
<p>运用这种规则直到收敛，就是批梯度下降算法 <code>BGD（Batch Gradient Descent）</code>，判断收敛的方法主要包括：一是,两次迭代后参数的变化量；二是,两次迭代后损失函数的变化量。<br>
规则中的\(\alpha\)为步长因子，又称为学习率，需要在实践中调整，过小会导致算法收敛的很慢，过大会导致算法很容易越过最优点，或在最优点附近震荡。<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_梯度下降的局限性">2.1.3. 梯度下降的局限性</h4>
<div class="paragraph">
<p>梯度下降算法会导致 <code>局部极小值</code> 的产生，可以通过随机初始化，寻找多个最优点来解决该问题，在所有最优点中选择最小的作为最终的最终结果。对于本例中的线形回归问题，不会存在局部极值的问题，因为本问题的随时函数是凸二次函数。<br></p>
</div>
<div class="paragraph">
<p>根据公式(2.9)的更新规则，每次迭代更新都需要遍历所有样本，当样本量很大时算法运行速度就会变成龟速。<br></p>
</div>
<div class="paragraph">
<p>一种比较好的解决方案就是每次更新时我们只选用一个样本，由于一个样本的梯度方向是随机的，不是全局梯度方向，因此该方法又称为随机梯度下降 <code>SGD（Stochastic Gradient Descent）</code>； <code>BGD</code> 每次使用的样本过多，<code>SGD</code> 每次使用的样本又过少，每次更新时我们还可以随机选择一小批样本进行更新，这种方法叫做小批量梯度下降 <code>Mini-Batch Gradient Descent(MBGD)</code>。<br></p>
</div>
<div class="exampleblock">
<div class="title">示例 3. 扩展问题</div>
<div class="content">
<div class="ulist square">
<ul class="square">
<li>
<p>线形回归的损失函数为什么要选用最小二乘损失（<code>L2损失</code>），如何概率解释 ?</p>
</li>
<li>
<p>如何解决模型在训练集上过拟合问题 ?</p>
</li>
<li>
<p>如何从贝叶斯角度深入理解正则化,为什么说L1正则化等价于参数的先验概率分布满足拉普拉斯分布？L2正则化等价于参数的先验概率分布满足高斯分布 ?</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_深度思考_2">2.1.4. 深度思考</h4>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>1、先来看下第一个问题：<code>最小二乘法的概率解释</code>，即为什么选择平方函数作为目标函数会使得效果比较好(并非一定是)!</p>
</div>
<div class="ulist">
<ul>
<li>
<p>假设1:对每一个样本\((x^{i}, y^{i})\)，模型预测结果和真实结果的关系可以写成：<br></p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[y^{i} = \theta^{T}x^{i} + \epsilon^{i} \hspace{2cm} (1)\]
</div>
</div>
<div class="paragraph">
<p>其中，\(\epsilon^{i}\)表示模型的误差。<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>假设2:误差\(\epsilon^{i}\)服从正态分布，即<br></p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[\epsilon \sim N(0, \sigma^{2})  \hspace{2cm} (2)\]
</div>
</div>
<div class="paragraph">
<p>假设1只是一种表示形式，那么假设二为何会成立呢？这是因为影响误差的因素有很多，这些因素都是随机分布，根据中心极限定理，即 <code>许多随机变量的和趋向于正态分布</code> ，我们可以得到假设二，那么<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(\epsilon^{i}) = \frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(\epsilon^{i})^{2}}{2\sigma^{2}}) \hspace{1cm}(3)\]
</div>
</div>
<div class="paragraph">
<p>这也表示，当给定参数\(\theta\)和\(x\)时，目标值\(y\)也服从正态分布.<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(y^{(i)} \mid x^{(i)};\theta) = \frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(\theta^{T}x^{(i)}-y^{(i)})^{2}}{2\sigma^{2}}) \hspace{1cm} (4)\]
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>假设3:对于各个样本的误差\(\epsilon^{i}\)是独立同分布的随机变量，这样，我们可以得到似然函数<br></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>\begin{align}
l(\theta) &amp; = P(Y \mid X; \theta ) \\
&amp; = \prod_{i=1}^{m} p(y^{(i)} \mid x^{(i)};\theta) =\prod_{i=1}^{m} \frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(\theta^{T} x^{(i)} -y^{(i)} )^{2} }{2\sigma^{2}})  \hspace{1cm} (5)
\end{align}</p>
</div>
<div class="paragraph">
<p>似然函数什么意义呢？表示是在参数\(\theta\)下，数据集出现的概率，似然函数同概率的概念很相似，不同之处在于似然函数把\(\theta\)作为变量，找到使得数据集出现的概率最大时的参数，就是最大似然估计。<br></p>
</div>
<div class="paragraph">
<p>对于公式(5)的右端取对数，我们发现可以将似然函数最大化的问题转化为使得平方和最小的问题：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
L(\theta) &amp; = log \hspace{0.1cm} l(\theta) = log\prod_{i=1}^{m} \frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(\theta^{T} x^{(i)} -y^{(i)} )^{2} }{2\sigma^{2}}) \\
&amp; = -mlog\sqrt{2\pi }\sigma-\frac{1}{2\sigma^{2} }\sum_{i=1}^{m}( \theta^{T} x^{i} -y^{i}) ^{2} \\
\Rightarrow  \hspace{1cm} &amp; max(L(\theta)) \propto min( \frac{1}{2}\sum_{i=1}^{m} (\theta^{T} x^{i} -y^{i} )^{2} )
\end{align}</p>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>2、 再来看下第二个问题，其实可以简单解释为：过拟合是经验风险最小化策略带来泛化能力较差的问题，解决方法就常用的就是加正则化项，即</p>
</div>
<div class="ulist">
<ul>
<li>
<p>采用结构风险最小化的模型选择策略</p>
</li>
<li>
<p>除此之外，结合具体业务，可能还需要把精力放在训练样本的选择上，比如所选样本在某个特征上与真实数据的分布严重不一致，也可能会造成过拟合的问题</p>
</li>
<li>
<p>交叉验证可以解决过拟合问题，这也是一种非常重要的模型选择策略</p>
</li>
<li>
<p>具体到不同类型模型，还有提前终止、dropout、调整网络结构等.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>3、第三个问题可以参考第一章的思考题，这里我们可以简单给出结果形式<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>首先什么是拉普拉斯分布，当先验概率满足拉普拉斯分布就是:</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[p(w_{i}) = N(w_{i} \mid \mu , b) = \frac{1}{2b} e^{-\frac{|w_{i}-\mu|}{b}}\]
</div>
</div>
<div class="paragraph">
<p>损失函数取后验概率的负值，可以得到<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
w^{*} &amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w)-log \hspace{0.1cm} p(w) \\
&amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w)-\sum_{j=1}^{n}log \hspace{0.1cm} p(w_{j}) \\
&amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w) + \sum_{j=1}^{n}\frac{1}{b}|w_{j} - \mu| \\
&amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w) + \lambda \sum_{j=1}^{n}|w_{j}| \hspace{1cm} (\mu = 0, b = \frac{1}{\lambda})
\end{align}</p>
</div>
<div class="paragraph">
<p>我们看到目标函数的最后一样刚好就是 <code>L1</code> 正则化项，我们从假设参数先验分布为拉普拉斯分布，直接推出了 <code>L1</code> 正则化项，所以 <code>L1</code> 正则化等价于参数的先验分布满足拉普拉斯分布。
同理，让我么看下参数先验分布为高斯分布的情况<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>当参数的先验概率满足高斯分布时,相当于:</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[p(w_{i}) = N(w_{i} \mid \mu, \sigma^{2})=\frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(w_{i}-\mu)^{2}}{2\sigma^{2}})\]
</div>
</div>
<div class="paragraph">
<p>所以，最优参数为<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
w^{*} &amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w)-log \hspace{0.1cm} p(w) \\
&amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w)-\sum_{j=1}^{n}log \hspace{0.1cm} p(w_{j}) \\
&amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w) + \sum_{j=1}^{n} \frac{1}{\sigma^{2}}(w_{j} - \mu)^{2} \\
&amp; = \underset{w}{argmin}-log\sum_{i=1}^{m}p(D_{i} \mid w) +\lambda \sum_{j=1}^{n} (w_{j})^{2} \hspace{1cm}  (\mu = 0, \sigma = \sqrt{\frac{1}{\lambda}})
\end{align}</p>
</div>
<div class="paragraph">
<p>对比下式<br></p>
</div>
<div class="stemblock">
<div class="content">
\[w^{*} = \underset{w}{argmin}\sum_{i}L(y_{i}, f(x_{i}; w)) + \lambda\Omega (w)\]
</div>
</div>
<div class="paragraph">
<p>可以看出，似然函数部分对应损失函数(经验风险),而先验概率对应于正则化项，因此 <code>L2</code> 正则化等价于模型参数的先验概率满足正态分布。</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_逻辑回归">2.2. 逻辑回归</h3>
<div class="sect3">
<h4 id="_揭开面纱">2.2.1. 揭开面纱</h4>
<div class="paragraph">
<p>这一节将介绍在工业界应用最广，又最简单、最容易理解的神器 <code>逻辑回归(Logistic Regression)</code>，问题的背景：假设我们需要解决一个二分类问题，比如给你一辆ofo小黄车，判定该车是好车还是坏车。<br></p>
</div>
<div class="paragraph">
<p>首先，我们期望学到一个什么样的模型？最简单的需求就是把一批车辆的数据丢给模型，模型返回 <code>0</code> 或者 <code>1</code> 表示好和坏；更近一步，我们希望模型能返回给我们一个 <code>0~1</code> 之间的概率值，这样我们可以根据模型输出的概率值选择坏的概率很大的丢给师傅去修理，选择好的概率很大的推荐给用户骑行。<br></p>
</div>
<div class="paragraph">
<p>那么，首先明确了学习模型的输入和输出，输入是车辆的各种维度的信息（比如，昨天发生了几个正常单、报修单、报修部位、报修后是否又有正常骑行等等），输出是一个 <code>0～1</code> 之间的概率值。问题就变成了如何将输入转为输出。<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_sigmoid函数">2.2.2. Sigmoid函数</h4>
<div class="paragraph">
<p>输入空间显然是实数空间，输出空间为 <code>0～1</code> ，而 <code>sigmoid</code> 刚好就是可以完成这种归一化的函数：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[h(z)=\frac{1}{1+e^{-z}} \hspace{6cm} (2.10)\]
</div>
</div>
<div class="paragraph">
<p>而我们的输入可以表示为多个特征加权和的形式：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[z = \theta^{T} x \hspace{7.5cm} (2.11)\]
</div>
</div>
<div class="paragraph">
<p>有了这个映射函数，对于一个样例，我们就可以得到它分类的概率值：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(y=1|x;\theta) &amp; = h_{\theta}(x) \\
p(y=0|x;\theta) &amp; = 1 - h_{\theta}(x)
\end{align}</p>
</div>
<div class="paragraph">
<p>将上面两个公式联合起来可以写成：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(y|x;\theta) = [h_{\theta}(x)]^{y} [1-h_{\theta}(x)]^{1-y} \hspace{2cm} (2.12)\]
</div>
</div>
<div class="paragraph">
<p>这样我们就可以得到在整个数据集上的似然函数:<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
l(\theta) &amp; = p(Y|X;\theta) \\
&amp; = \prod_{i=1}^{m} p(y^{i}; x^{i}; \theta ) \\
&amp; = \prod_{i=1}^{m} [h_{\theta} (x^{i}) ]^{ y^{i} } [(1-h_{\theta} (x^{i})) ]^{1- y^{i} }\hspace{1cm}(2.13)
\end{align}</p>
</div>
<div class="paragraph">
<p>对似然函数取对数，可以得到：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[L(\theta) = log(l(\theta)) = \sum_{i=1}^{m}[y^{i} logh_{\theta}(x^{i}) + (1 - y^{i}) log(1-h_{\theta}(x^{i}))] \hspace{0.5cm}(2.14)\]
</div>
</div>
<div class="paragraph">
<p>为了简化其间，我们先只考虑一个样本的情况，则：</p>
</div>
<div class="stemblock">
<div class="content">
\[L(\theta) = ylogh_{\theta}(x) + (1-y)log(1-h_{\theta}(x)) \hspace{1cm} (2.15)\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_参数更新">2.2.3. 参数更新</h4>
<div class="paragraph">
<p>公式(2.15)是二分类问题的最大似然函数，那么我们应该怎样定义损失函数，然后应用梯度下降更新参数呢？首先，损失函数作为优化目标时，其函数值越小越好；而似然函数则刚好相反，其越大越好。所以一个很自然的想法就是取似然函数的负函数作为损失函数：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[J(\theta) = -L(\theta) = -ylogh_{\theta}(x) - (1-y)log(1-h_{\theta}(x)) \hspace{2cm} (2.16)\]
</div>
</div>
<div class="paragraph">
<p>对公式(2.16)应用梯度下降算法，更新规则为:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} - \frac{\partial}{\partial \theta_{j}}J(\theta) \hspace{6cm} (2.17)\]
</div>
</div>
<div class="paragraph">
<p>求公式(2.16)的导数，公式中有一个复合函数\(h_{\theta}(x)\)，我们可以先求解它的导数，求解过程如下：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\frac{\partial}{\partial h_{\theta}(x_{j})} &amp; = (\frac{1}{1+e^{-{ \theta^{T} } x}})^{'} \\
&amp; = [(1 + e^{- \theta^{T} x})^{-1} ]^{'} \\
&amp; = (-1) (1 + e^{- \theta^{T} x})^{-2} e^{- \theta^{T} x} (- x^{j} ) \\
&amp; = x_{j} ·\frac{ e^{- \theta^{T} } x}{ (1 + e^{- \theta^{T} x} )^{2} } \\
&amp; = x_{j} ·\frac{ 1+ e^{- \theta^{T} x} -1}{ (1 + e^{- \theta^{T} x} )^{2} } \\
&amp; = x_{j}· [( h_{\theta}(x) )^{2} - h_{\theta}(x)] \\
&amp; = x_{j}· h_{\theta}(x)(1 - h_{\theta}(x) ) \hspace{3cm} (2.18)
\end{align}</p>
</div>
<div class="paragraph">
<p>实际上，<code>sigmoid</code> 函数 <code>y=h(x)</code> 的导数就等于 <code>y(1-y)</code>。<br>
然后求解损失函数\(J(\theta)\)的导数，利用公式(2.18)的结论，求解过程如下:<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\frac{\partial}{\partial \theta_{j}}J(\theta) &amp; = -[ ylogh_{\theta}(x) + (1-y)log(1-h_{\theta}(x)) ]^{'} \\
&amp; = -[ \frac{y}{h_{\theta}(x)} h_{\theta}^{'}(x) + \frac{1-y}{1-h_{\theta}(x)} (-h_{\theta}^{'}(x) ] \\
&amp; = -x_{j} [ \frac{y}{h_{\theta}(x)} h_{\theta}(x) (1-h_{\theta}(x)) - \frac{1-y}{1-h_{\theta}(x)} h_{\theta}(x)(1-h_{\theta}(x))] \\
&amp; = -x_{j}[y(1-h_{\theta}(x))-(1-y)h_{\theta}(x)] \\
&amp; = -x_{j}[y-yh_{\theta}(x)+yh_{\theta}(x)-h_{\theta}(x)] \\
&amp; = x_{j}(h_{\theta}(x) - y) \hspace{6cm} (2.19)
\end{align}</p>
</div>
<div class="paragraph">
<p>导入公式(2.17),得到参数更新的规则为:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} - \alpha · (h_{\theta}(x)-y)·x_{j} \hspace{3cm} (2.20)\]
</div>
</div>
<div class="paragraph">
<p>考虑多个样本的时候，规则就应该变成:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\theta_{j} = \theta_{j} - \alpha · \sum_{i=1}^{m}(h_{\theta}(x^{i})-y^{i})·x_{j}^{i} \hspace{2cm} (2.21)\]
</div>
</div>
<div class="exampleblock">
<div class="title">示例 4. 扩展问题</div>
<div class="content">
<div class="ulist square">
<ul class="square">
<li>
<p>为什么二分类问题的模型会叫 <code>逻辑 <em>回归</em></code> ?</p>
</li>
<li>
<p>损失函数不选用公式(2.16)的形式会怎样？比如，仍然采用最小二乘法。</p>
</li>
<li>
<p>转换函数还有别的选择吗？为什么要选择 <code>sigmoid</code> 函数？</p>
</li>
<li>
<p>回归问题通过一个非线性变换就变成了分类问题，从拟合数据转变为拟合决策边界，这是什么原因导致的？</p>
</li>
<li>
<p>如果损失函数就是 <code>L2</code> 损失，有什么办法可以求解？</p>
</li>
<li>
<p>最大似然估计和最小化损失函数、最大后验概率等是什么关系？</p>
</li>
<li>
<p>如何用逻辑回归解决多分类问题？</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_深度思考_3">2.2.4. 深度思考</h4>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>1、 先来看下第一题，为什么叫 <code>逻辑回归</code> ?<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>简单点回答将 <code>logistic</code> 直译为中文就是了，其实这个模型的准确名字应该叫 <code>对数几率模型</code>，因为这个模型( <code>sigmoid函数</code>)可以由正负样本可能性比值的对数推出。</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>\begin{align}
f(x; \theta) &amp; = ln\frac{p(y=1 \mid x)}{p(y=0 \mid x)} \\
 &amp; = ln\frac{p(y=1 \mid x)}{1- p(y=1 \mid x)} \\
 &amp; = \theta^{T} x
\end{align}</p>
</div>
<div class="paragraph">
<p>由上面公式可以推出<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\frac{p(y=1 \mid x)}{1- p(y=1 \mid x)} &amp; = e^{ \theta^{T} x} \\
p(y=1 \mid x) &amp; = (1-p(y=1 \mid x)) e^{ \theta^{T} x} \\
(1+e^{ \theta^{T} x}) p(y=1 \mid x) &amp; = e^{ \theta^{T} x} \\
p(y=1 \mid x) &amp; = \frac{e^{ \theta^{T} x} }{ 1+e^{ \theta^{T} x}} \\
&amp; = \frac{1}{1 + e^{ -\theta^{T} x} }
\end{align}</p>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>2、逻辑回归的损失函数如果选择均方差损失会怎样呢 ？ <br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>均方差损失的函数形式为</p>
</li>
</ul>
</div>
<div class="stemblock">
<div class="content">
\[J(\theta) = \frac{1}{m}\sum_{i=1}^{m} (h_{\theta}(x_{i})-y_{i}))^{2}\]
</div>
</div>
<div class="paragraph">
<p>在逻辑回归中我们的\(h_{\theta}(x)\)形式为:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[h_{\theta}(x) = \frac{1}{1 + e^{-\theta^{T} x}}\]
</div>
</div>
<div class="paragraph">
<p>代入损失函数，我们可以得到如下图的损失函数。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/lr_mse.png" alt="lr mse" width="50%" height="55%">
</div>
<div class="title">图 1. 具有众多局部极小值的损失函数</div>
</div>
<div class="paragraph">
<p>这样的损失函数，我们使用梯度下降，会很容易收敛到局部最小值，而不是全局最小值。</p>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>3、为什么要选择 <code>sigmoid</code> 函数作为损失函数 ？ 关于这个问题，你会在网上搜到很多解释和答案，但大部分都在说 <code>sigmoid</code> 函数的数学特性有多好，可以给出预测结果的概率解释等，其实都是在说为什么可以用而没有解释为什么必须用的问题。<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>说到底，源于指数分布簇所特有的最佳性质-最大熵理论，即指数分布簇的最大熵等价于其指数形式的最大似然界</p>
</li>
<li>
<p>二项式的最大熵解等价于二项式指数形式(<code>sigmoid</code>)的最大似然，多项式分布的最大熵等价于多项式分布指数形式(<code>softmax</code>)的最大似然</p>
</li>
<li>
<p>因此为什么要用 <code>sigmoid</code>，那是指数分布簇最大熵特性的必然性，这部分的解释，我们将在第7章 <code>EM</code> 模型给出。</p>
</li>
</ul>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>4、这道题很简单，人工智能也好，机器学习也好，原理都是人让它干啥，它就干啥，而决策边界其实就是模型，它是什么完全是由人定义的损失函数决定的。</p>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>5、这个问题非常有深度，在这里谈谈我自己的一些想法，欢迎多给宝贵想法和建议。<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>首先，目前讲到的优化算法都是基于梯度下降，朝损失函数变小的方向，不断更新参数，更新策略也是基于步长因子和负梯度综合考虑；</p>
</li>
<li>
<p>算法收敛的依据无非是损失或者参数是否基本不变等，可是现在的问题是有很多极小值，就像你下坡一样，山上有很多山谷，你并不知道哪个是最低的山谷；</p>
</li>
<li>
<p>文章正文在分析梯度下降缺点的时候，给了一些参考的解决方案，如选择多个初始位置，找到多个极小值，取最小的那个，这种方式都是比较传统的方法，也有自身的问题，比如训练代价变得更高，应该初始化几次合适等等；</p>
</li>
<li>
<p>最近在思考有没有一种模型可以直接解决这个问题，想到了 <code>残差</code>，这个概念会在决策树和集成学习那个章节讲到，这里既然提到这个问题，可以大致说下我的想法;</p>
</li>
<li>
<p>在梯度提升算法中，大家最常见的应该就是GBDT，其实这个集成学习的基模型不一定非要是树模型，也可以是线形模型，比如逻辑回归;</p>
</li>
<li>
<p>除了GBDT还有一个类似的模型叫GBR，叫梯度提升回归模型，这里先说一个后面会讲到的结论，当损失函数为平方差损失时，负梯度其实就等于 <code>残差</code>,这个很容易证明，对平方差函数求导就可以得出结论；</p>
</li>
<li>
<p>那一种解决方案是不是可以像 <code>AdaBoost</code> 那样，学习多个逻辑回归模型，后面的模型学习是基于之前模型的 <code>残差</code> 为优化目标(最小化)!</p>
</li>
</ul>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>6、这个问题其实没什么技术含量，主要是大家在学习的过程中会经常遇到这些类似的概念，单独拿出来作为思考题，主要是为了加深对这些概念的理解，做到所学知识的融会贯通.<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>首先，我们先说下 <code>最小化损失函数</code>,我们在第一章讲过，模型选择的关键依据就是选择损失函数最小的模型，表现出来的就是经验风险最小化的思想，任何其他最大化**的问题，只要取负值，都可以很容易的将优化目标转化为损失函数;</p>
</li>
<li>
<p>其次，最大似然估计是一种比较传统的参数学习方法, 和最小化损失函数一样，都是为了求解最优参数的值，将似然函数取负就可以作为损失函数，然后就变成了最小化损失函数的问题，一般的似然函数都是对数似然，对应的损失函数就是log损失，其实是在做同一件事；</p>
</li>
<li>
<p>最后，最大后验概率估计是贝叶斯思想的一种体现，对应的模型都属于概率模型，因此可以用贝叶斯公式将优化问题转换为似然函数和先验概率的乘积，因此最大后验概率估计其实就是最大似然估计加上正则化项(对应先验概率),当然将后验概率取负又可以转为损失函数的最小化问题；</p>
</li>
<li>
<p>由此可见，上述几个概念除了通用和特例之间的差异外，都是基于最优化问题求解参数的最优解，没有太大本质的差异。</p>
</li>
</ul>
</div>
</div>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>7、逻辑回归是典型的二分类问题，如果将它应用于多分类问题呢？ 这里直接给出吴恩达老师的答案。<br></p>
</div>
<div class="ulist">
<ul>
<li>
<p>将多个类型看作两个类型，比如类型A和非A，学习出类型A的逻辑回归模型;</p>
</li>
<li>
<p>然后在对非A的类型执行上述操作即可。</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_特征工程">3. 特征工程</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_引子">3.1. 引子</h3>
<div class="paragraph">
<p>首先来看一张关于梯度下降的示意图：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/feature_scaling.png" alt="feature scaling" width="60%" height="45%">
</div>
<div class="title">图 2. 梯度下降</div>
</div>
<div class="paragraph">
<p>上图给出了两个损失函数的等高线，左图是一个很扁的等高线，右图是一个很规整的等高线，哪一个损失函数好？<br></p>
</div>
<div class="paragraph">
<p>一个很直观的结论是：当在左图进行梯度下降时，如果我们选择的初始位置为长轴附近，那么需要很多次的迭代才能到达最优点，现象就是算法收敛的很慢，模型学不动。由于模型学习时初始位置都是随机选取的，因此我们更倾向于选择第二种损失函数。<br></p>
</div>
<div class="paragraph">
<p>怎样才能保障我们的损失函数像右图那样规整呢？这就是特征工程的一个典型作用，当然特征工程的作用还有很多，以至于它是非常非常重要！<br></p>
</div>
</div>
<div class="sect2">
<h3 id="_特征工程_2">3.2. 特征工程</h3>
<div class="paragraph">
<p>下图是一个机器学习任务典型的工作流程:<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/pipline.png" alt="pipline" width="60%" height="45%">
</div>
<div class="title">图 3. 工作流程</div>
</div>
<div class="paragraph">
<p>这个 <code>pipline</code> 包括了数据预处理、模型学习、模型评估和新样本预测，每个过程的时间和收益比是按照流程越来越低的，也就是我们把更多的时间花在数据预处理上，获得的收益远大于模型算法的选择；换言之，如果数据处理有问题，后面的环节再完美，也无法得到不错的模型。<br></p>
</div>
<div class="paragraph">
<p>工业界，大概有70%的时间都花在了数据预处理上，当然数据预处理中又包含了数据清洗、特征工程等。一般，算法工程师拿到的数据是经过数据挖掘或数据分析师们清洗后的数据，因此更重要的工作是进行特征工程！<br></p>
</div>
<div class="paragraph">
<p>不过有的时候，算法工程师们也需要从头做起，数据清洗往往是第一步，我们就从数据清洗开始讲述特征工程。<br></p>
</div>
<div class="sect3">
<h4 id="_数据清洗">3.2.1. 数据清洗</h4>
<div class="paragraph">
<p><code>Garbage in , garbage out !</code> 当你给模型丢进一堆错误数据时，很显然你得到的也肯定是毫无意义的结果;<br></p>
</div>
<div class="paragraph">
<p>算法大多数情况下就是个加工厂，至于最后的产品(输出)如何，取决于原材料的好坏:<br></p>
</div>
<div class="paragraph">
<p>这个过程会花掉你一般的时间，当然会促进你对业务的理解;<br></p>
</div>
<div class="paragraph">
<p>数据清洗要做的事情就是去掉脏数据！<br></p>
</div>
<div class="paragraph">
<p>比如，一个人的身高为3米&#8230;&#8203;    缺省值太多的样本丢弃&#8230;&#8203;    数据间存在相互矛盾&#8230;&#8203;<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_数据采样">3.2.2. 数据采样</h4>
<div class="paragraph">
<p>很多情况下数据样本是不均衡的，如小黄车中好车和坏车的比例<br></p>
</div>
<div class="paragraph">
<p>往往好车的数量要远远大于坏车的数量，这个时候该如何采样？<br></p>
</div>
<div class="paragraph">
<p>如果坏车的数量也很多，那就下采样；如果坏车的数量很有限，多采集、过采样(旋转等)、修改损失函数<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_特征处理">3.2.3. 特征处理</h4>
<div class="sect4">
<h5 id="_数值型特征">3.2.3.1. 数值型特征</h5>
<div class="ulist">
<ul>
<li>
<p>幅度调整/归一化 如，各种 <code>scalar</code></p>
</li>
<li>
<p><code>log</code> 等变换</p>
</li>
<li>
<p>换成统计值，如 <code>max、min、mean、std</code></p>
</li>
<li>
<p>离散化、<code>Hash</code> 分桶</p>
</li>
<li>
<p>尝试转为类别型特征</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_类别型特征">3.2.3.2. 类别型特征</h5>
<div class="ulist">
<ul>
<li>
<p><code>One-hot</code> 编码, [0 0 0 1 0]</p>
</li>
<li>
<p>哑变量，虚拟变量，如没有填写性别的用户由于数据量较大，给他们一个类别标识</p>
</li>
<li>
<p><code>Hash</code> 与聚类处理，如海量数据推荐，不会直接比较每个数据的相似性，往往会先进行聚类</p>
</li>
<li>
<p>尝试转为数值型</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_时间型特征">3.2.3.3. 时间型特征</h5>
<div class="ulist">
<ul>
<li>
<p>看作连续值，如转为持续时间或间隔时间</p>
</li>
<li>
<p>看作离散值，如一天中的哪个时段、一周中的星期几、一年中的哪个月、是不是周末、是不是假期等</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_文本型特征">3.2.3.4. 文本型特征</h5>
<div class="ulist">
<ul>
<li>
<p>词袋，去掉停用词后，在词库中的映射稀疏向量</p>
</li>
<li>
<p><code>N-gram</code></p>
</li>
<li>
<p>使用 <code>TF-IDF</code> 统计<br>
TF(t) = (词t在当前文中出现次数) / (t在全部文档中出现次数)<br>
IDF(t) = ln(总文档数/ 含t的文档数)<br></p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_统计特征">3.2.3.5. 统计特征</h5>
<div class="ulist">
<ul>
<li>
<p>多维度统计特征，具备良好特征的潜质</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="_组合特征">3.2.3.6. 组合特征</h5>
<div class="ulist">
<ul>
<li>
<p>一般用树模型进行组合特征的筛选，一条组合路径就是一个组合特征</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_支持向量机">4. 支持向量机</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_主题模型">5. 主题模型</h2>
<div class="sectionbody">
<div class="paragraph">
<p>如果你已经对 <code>LDA</code> 模型研究了很久，看过Rickjin的 <code>LDA数学八卦</code> ，但是你仍然对其中的细节不是很理解，无法将所有知识串联起来，你可以直接跳到5.3.6节，那里梳理的思路中将所有涉及的知识进行了串联，希望可以解决你的困惑！<br></p>
</div>
<div class="sect2">
<h3 id="_解决什么问题">5.1. 解决什么问题</h3>
<div class="paragraph">
<p>主题模型主要应用于自然语言处理领域，其核心应用正如其名，可以用来进行文章的主题发现，信息的语义提取，生成文章摘要，更进一步可以进行社区的挖掘。为了更好地说明本节主题内容，这里可以假定我们要处理地应用为 <code>文章主题发现</code>。<br></p>
</div>
<div class="paragraph">
<p>文章主题发现的模型有很多，我个人比较喜欢LDA(Latent Dirichlet Allocation，隐狄利克雷分布)模型，它在文本建模中是一个非常优雅的模型，
刚开始学习这个模型的时候，读了Richjin的《LDA数学八卦》，被文章中涉及的数学之美深深吸引，可能是由于篇幅过长的原因，读完感觉很爽，
却无法将所有内容串联起来，每部分的内容都可以看懂，到最后却没能领悟到如此简单的实现过程中，那些数学的美都藏在何处？<br></p>
</div>
<div class="paragraph">
<p>个人感觉，学习LDA的门槛并不低，想要让大家都能理解也绝非易事。我反复阅读了许多相关博客和论文，等到拨开云雾见青天的时候，觉得很有必要将自己的领悟和遇到的坑都记录下来，
也可能是自己能力有限，无法用简短的语言将这个模型讲清楚，本节内容会比较多，我将尽我所能，从先验我本人为一个小白开始，不断假设、追问，又不断找到答案，
从一个正常人的思路，循序渐进讲述一个小白都可以理解的LDA，希望对大家有所助益。<br></p>
</div>
</div>
<div class="sect2">
<h3 id="_给问题建模">5.2. 给问题建模</h3>
<div class="paragraph">
<p>我们的目的是想训练一个主题模型，它可以自动完成新文章主题的发现，也就是说当我们将一篇文章输入给训练好的主题模型后，它可以返回该文章对应的 <code>主题分布</code>，
比如：我们将 <code>天龙八部</code> 这本书(假定为一篇很长的文章)作为模型的输入，则模型返回的结果为：[60%:武侠，30%:爱情，0%:计算机, 5%:教育&#8230;&#8203;],
返回结果显示 <code>天龙八部</code> 属于武侠爱情小说的概率很高，这比较符合我们的预期，所以对应的我们可以说主题模型学的不错，而如果返回的结果显示90%的是计算机，
那将是一个很糟糕的模型。<br></p>
</div>
<div class="paragraph">
<p>上面的例子看上去很好理解，但是有个问题大家有没有考虑到：那些主题(武侠、爱情、计算机等)是哪里来的？其实，在模型训练时，我们丢给模型的是一堆文章，
这些文章的主题是什么我们也不知道，因此这个模型是一个无监督的模型，我们希望计算机可以帮我们自动发现主题，但这里的主题是数字，不能包含具体的含义。
打个比方，我们认为所有的文章共有K个主题，那么我们将主题进行编号 \(Z_{i}\) 表示第\(i\)个主题，模型可以返回文章对应每个主题编号的概率，
我们可以按照文章概率最大的主题编号对文章进行分类，这样就可以将文章分为\(k\)类，简单看下各类文章的内容，可以很容易的将文章分为：新闻、体育、教育等，
这不就是基于内容的新闻网站的典型应用么。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/miss_topic.png" alt="miss topic" width="30%" height="35%">
</div>
<div class="title">图 4. 跑题</div>
</div>
<div class="paragraph">
<p>在继续阅读前，大家可以想一个问题，我们的模型是否真的可以学出来？我个人任务这个思考很有意义，而且我也认为一定是可以学的出来，大家可以想下：文章是如何产生的？
思考下我们上学时写作文的场景，在一篇作文开始书写前，我们首先看到的是作文的要求，比如写一篇 <code>我的爸爸是&#8230;&#8203;</code>，很显然这是一篇写人的文章，
然后有了这个主题后，我们的作文里的词就会围绕这个主题展开，很难想象在写这篇作文的时候你会用到 <code>百合花</code> 这种写物的词。简单点说，文章是由一个个词构成的，
但是每个文章的词又是由这篇文章的主题构成的，当老师给你作文打了0分，并批注 <code>跑题</code> 了的时候，就说明你文章里那800多个词真的有可能是 <code>百合花</code>，
其实我们模型学习的目的就是希望它可以像老师一样发现你作文的主题，很显然这个事情是可以做到的。<br></p>
</div>
<div class="sect3">
<h4 id="_建模">5.2.1. 建模</h4>
<div class="paragraph">
<p>假定：我们提供的文章数目为\(M\)，称为 <code>预料</code>，预料中所有不重复的词共有\(V\)个，假定所有文章共包含的主题有\(K\)个，用\(d\)表示某个文档，\(k\)表示主题，
\(w\)表示词汇,\(n\)表示词，下面的定义完全不用care。<br></p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>\(z[d][w]\)：第\(d\)篇文章的第\(w\)个词来自哪个主题，\(M\)行，\(X\)列，\(X\)为相应文档的长度，即词(可重复的数目</p>
</li>
<li>
<p>\(nw[w][t]\)：第\(w\)个词是第\(t\)个主题的次数， <code>word-topic</code> 矩阵，列向量\(nw[][t]\)表示主题\(t\)的词频分布，共\(V\)行，\(K\)列</p>
</li>
<li>
<p>\(nd[d][t]\)：第\(d\)篇文章中第\(t\)个主题出现的次数， <code>doc-topic</code> 矩阵，行向量\(nd[d]\)表示文档主题频数的分布，共\(M\)行，\(K\)列</p>
</li>
</ul>
</div>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/topic_word_article.png" alt="topic word article" width="50%" height="55%">
</div>
<div class="title">图 5. 跑题</div>
</div>
</div>
<div class="sect3">
<h4 id="_第一次思考">5.2.2. 第一次思考</h4>
<div class="exampleblock">
<div class="title">示例 5. 脑洞大开</div>
<div class="content">
<div class="ulist square">
<ul class="square">
<li>
<p>训练的目标是什么? 如何抽象这个问题?</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>为了回答这个问题，首先想下我们有什么，我们有一个矩阵，\(M*N\)的矩阵，\(N\)是一个向量，元素\(N_{i}\)表示第\(i\)篇文章的长度(词个数)。
我们希望通过训练让计算机帮我们自动获得预料库中每篇文章的主题，<code>如何表征一篇文章的主题？文章的主题不是显示存在的，是隐藏在文章的所有词背后的隐变量。</code><br>
假定主题个数\(K=5\)，分别为 <code>爱情、武侠、教育、音乐、体育</code> ，如果可以统计出文章对应这5个主题的概率就可以了，怎么统计？一个很直观的想法就是统计出文章的所有\(N_{i}\)个词对应主题的个数。<br></p>
</div>
<div class="paragraph">
<p>例如：我们有两篇文章，第一篇文章为：“小龙女教杨过武功，后来爱上了杨过”；第二篇文章为：“学习英语重在培养语感，多听英文歌曲可以培养语感”。<br>
如何进行统计呢？每篇文章的词是已知，但是主题我们并不知道，只知道个数，假定我们已知每个主题对应的词分布，比如：</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>爱情：爱、爱上</p>
</li>
<li>
<p>武侠：小龙女、杨过、武功、教</p>
</li>
<li>
<p>教育：教、学习、英语、语感、培养、英文</p>
</li>
<li>
<p>音乐：歌曲、听</p>
</li>
<li>
<p>体育：篮球、足球、运动·</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>那统计后的结果大致为：</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/article_topic_alloc.png" alt="article topic alloc" width="50%" height="55%">
</div>
<div class="title">图 6. 文章主题分布</div>
</div>
<div class="imageblock">
<div class="content">
<img src="images/article_topic_alloc_line.png" alt="article topic alloc line" width="50%" height="55%">
</div>
<div class="title">图 7. 文章主题分布-平滑</div>
</div>
<div class="paragraph">
<p>统计的过程比较简单，对文章进行分词，然后按照主题进行统计，上图可以看出第一篇文章的主题是武侠，第二篇文章的主题是教育，对于平滑后的分布图，可以看出最高点就是文章对应的主题，这里假设每个文章只有一个主题。
而平滑主题分布图中的最高点实际就是对应主题分布的 <code>期望</code>。对于上面的两张图的横坐标，我采用了两种形式表示，第一幅图是明文的主题，第二幅图是主题的编号，而实际计算机训练时是按照第二副图进行的，它并不知道主题具体是什么。<br></p>
</div>
<div class="paragraph">
<p>事情进展的很顺利，看上去主题发现是很简单的问题，只不过在上面的描述中我们有一个假定条件：<code>假定我们已知每个主题对应的词分布</code> 。如果这个分布已知，事情就会像上面那么简单顺利，因此问题的关键就转换为如何求解这个分布！</p>
</div>
<div class="paragraph">
<p><strong>[关键点01]</strong><br></p>
</div>
<div class="paragraph">
<p><code>如何求解主题对应的词分布？</code></p>
</div>
</div>
<div class="sect3">
<h4 id="_第二次思考">5.2.3. 第二次思考</h4>
<div class="paragraph">
<p>通过第一次的思考，我们终于把要解决的问题想清楚了，没错，就是要求解所有\(K\)个主题对应的词分布。<br>
.脑洞大开</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="ulist square">
<ul class="square">
<li>
<p>如何求解所有主题对应的词分布？</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>一个很大胆的想法就是，如果我们看到的不是每篇文章都写满了词，而是每篇文章中写满了词和对应主题编号，形如：[word:topic]，那么我们就可以统计出每个主题对应的词(统计每个主题下的词频即可)，当然也可以统计出每篇文章的主题分布。<br>
悲剧的是，开始训练时，我们对主题一无所知，只知道主题的个数\(K\)，还是拍脑袋定的！！！<br></p>
</div>
<div class="paragraph">
<p>那么，再来一个大胆的想法：<code>能不能我随便给每个词指定一个主题，然后通过不断迭代，最终所有词都可以收敛到它本应该对应的主题上呢？</code><br>
直觉告诉我们，这是有可能的，就像随机梯度下下降算法，不管我们初始位置选哪里，而且尽管每次我们都随机的挑选一个样本来更新参数，最后仍然可以收敛，关键在于如何定义损失，在这里就是如何找到一个合理的方向让算法迭代到收敛，毕竟每篇文章不是胡乱编写的，它背后是隐藏这一个明确主题的！<br></p>
</div>
<div class="paragraph">
<p><strong>[关键点02]</strong><br>
<code>能不能我随便给每个词指定一个主题，然后通过不断迭代，最终所有词都可以收敛到它本应该对应的主题上呢？</code></p>
</div>
<div class="paragraph">
<p>如果你之前了解过相关知识，我相信你应该能想到答案了，没错，马尔可夫链的平稳分布就具有这个特点：<code>不管初始状态是什么，经过有限次的迭代，最终收敛到一个稳定的分布</code>。我不敢假设所有人都有这个先验的知识，如果你不知道，那就让我来讲个故事，把马氏链引出来吧。<br></p>
</div>
<div class="paragraph">
<p>社会学家经常把人按照其经济状态分成3类：下层、中层和上层，我们用1、2、3分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层输入的概率为0.65，
属于中层收入的概率是0.28，属于上层收入的概率是0.07。事实上，从父代到子代，收入阶层变化的转移概率如下：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/transfer_prob.png" alt="transfer prob" width="50%" height="55%">
</div>
<div class="title">图 8. 收入阶层的转换概率</div>
</div>
<div class="paragraph">
<p>使用矩阵的表达方式，转换概率矩阵记为：</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/transfer_matrix.png" alt="transfer matrix" width="30%" height="35%">
</div>
<div class="title">图 9. 转换概率矩阵</div>
</div>
<div class="paragraph">
<p>假定当前一代人处于下层、中层和上层的人的比例是概率分布向量\(\pi_{0}=[\pi_{0}(1),\pi_{0}(2),\pi_{0}(3)]\),那么他们子女的分布比例将是\(\pi_{1}=\pi_{0}P\),他们孙子代的分布比例将是\(\pi_{2}=\pi_{1}P=\pi_{0}P^{2},&#8230;&#8203;\),
第\(n\)代子孙的收入分布比例将是\(\pi_{n}=\pi_{n-1}P=\pi_{0}P^{n}\)。<br></p>
</div>
<div class="paragraph">
<p>假设初始概率分布为\(\pi_{0} = [0.21, 0.68, 0. 11]\)，则我们可以计算前\(n\)代人的分布状态如下：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/alloc_example.png" alt="alloc example" width="30%" height="35%">
</div>
<div class="title">图 10. 前n代人的分布状态</div>
</div>
<div class="paragraph">
<p>我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然吗？我们换一个初始概率分布\(\pi_{0}=[0.75, 0.15, 0.1]\)试试看，继续计算前n代人的分布状况如下：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/alloc_example2.png" alt="alloc example2" width="30%" height="35%">
</div>
<div class="title">图 11. 前n代人的分布状态</div>
</div>
<div class="paragraph">
<p>我们发现到第9代的时候，分布又收敛了。最奇怪的是，两次给定不同的初始概率分布，最终都收敛到概率分布\(\pi=[0.286, 0.489, 0.225]\)，也就是说 <code>收敛的行为和初始概率分布无关</code>。走到这一步，你一定会惊叹：<br>
<strong> 发现上帝了，我们的问题可以求解了！ </strong></p>
</div>
<div class="sect4">
<h5 id="_一个重大的发现">5.2.3.1. 一个重大的发现</h5>
<div class="paragraph">
<p>为了突出惊叹，我们就另起一个小节吧，先把重大的发现记录下来：<br>
<code>概率分布收敛的行为和初始概率分布无关</code><br>
继续聊上面的例子，如果最终的分布同初始分布无关，那就说明主要是由状态转移矩阵\(P\)决定的，让我们计算下\(P^{n}\)<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/pn.png" alt="pn" width="50%" height="55%">
</div>
</div>
<div class="paragraph">
<p>我们发现当\(n\)足够大的时候，这个\(p^n\)矩阵的每一行都稳定的收敛到\(\pi=[0.286, 0.489, 0.225]\)这个概率分布。自然的这个收敛现象并不是马氏链独有的，
而是绝大多数马氏链的共同行为，关于马氏链的收敛性我们有个漂亮的定理。<br></p>
</div>
<div class="paragraph">
<p>在继续下去之前，我们需要重新更新下我们的问题：<strong>找到一个转移矩阵，使得我们随机指定每个词的主题，经过\(n\)轮迭代，最终所有词的主题分布会收敛到稳定分布，即合理分布。</strong><br></p>
</div>
<div class="paragraph">
<p><strong>定理01:</strong> 如果一个非周期马氏链具有转移概率矩阵\(P\),且它的任何两个状态都是联通的，那么\(\lim_{n \to \infty }P_{ij}^{n}\)存在且与\(i\)无关，记\(\lim_{n \to \infty }P_{ij}^{n}=\pi (j)\),我们有：<br></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>\(\lim_{n \to \infty }P^{n}=[\pi_{1} \pi_{2} &#8230;&#8203;\pi_{j}&#8230;&#8203;]\)<br></p>
</li>
<li>
<p>\(\pi (j) = \sum_{i=0}^{\infty }\pi(i)P_{ij}\)<br></p>
</li>
<li>
<p>\(\pi\)是方程\(\pi P=\pi\)唯一非负解,其中\(\pi=[\pi_{1}, \pi_{2},&#8230;&#8203;,\pi_{j},..]\)，\(\sum_{i=0}^{\infty }\pi(i)=1\)<br></p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>则\(\pi\)称为马氏链的平稳分布。<br>
这个马氏链的收敛定理非常重要，<code>所有的MCMC(Markov Chain Monte Carlo)方法都是以这个定理作为理论基础的</code>。 定理的证明比较复杂，一般的随机过程的课本中也不给出证明，
我们就不纠结于此了，直接用这个定理就好了。下面对于这个定理的内容做一些解释说明：<br></p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>该定理中马氏链的状态不要求有限，可以是无穷多个</p>
</li>
<li>
<p>定理中的 <code>非周期</code> 概念，我们不打算解释，因为我们遇到的绝大多数马氏链都是非周期的</p>
</li>
<li>
<p>两个状态\(i,j\)是联通的，并不是指\(i\)可以一步就转移到\(j\)，而是指有限步联通，马氏链的任意两个状态是联通的含义是指存在一个\(n\)，使得矩阵\(P^{n}\)中任何一个元素的数值都大于零。</p>
</li>
<li>
<p>由于马氏链的收敛行为，假定\(n\)步后收敛，则\(x_{n}, x_{n+1},&#8230;&#8203;\)都是平稳分布\(\pi_{x}\)的样本。</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>上面这些其实都不重要，重要的是你要想到：<code>我们的目标是一个概率分布，如果我们可以构造一个转移矩阵，使得马氏链的平稳分布刚好就是求解的分布，那么我们从任何一个初始状态出发，沿着马氏链转移，如果第n步收敛，则我们就得到了所求分布对应的样本</code>。<br></p>
</div>
<div class="paragraph">
<p>这个绝妙的想法在1953年被 <code>Metropolis</code> 想到了，<code>Metropolis</code> 考虑了物理学中常见的玻尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡洛方法，即 <code>Metropolis</code> 算法,<code>Metropolis</code> 算法是一个普适的采样方法，并启发了一系列 <code>MCMC</code> 方法，
所以人们把它视为随机模拟技术腾飞的起点。<code>Metropolis</code> 算法也被选为二十世纪十大最重要的算法之一。<br></p>
</div>
</div>
<div class="sect4">
<h5 id="_metropolis_hastings算法">5.2.3.2. Metropolis Hastings算法</h5>
<div class="paragraph">
<p>收下我们的小心思，想想我们的问题走到哪里了：<br>
<strong>我们最初是要求解每篇文章主题的概率分布，可以通过统计每个词对应主题的个数来近似估计，但是每个词对应主题我们也不知道，于是我们希望随便给每个词指定一个主题，通过迭代收敛到稳定的分布，即每个词应该对应的主题编号上，后来我们神奇地发现马氏链的平稳分布的性质可以应用于我们的问题求解中，关键在于如何获得这个转移矩阵！</strong><br></p>
</div>
<div class="paragraph">
<p>好，略轻思路，我们继续，我们目前关注的问题仍然是如何获得这个神奇的 <code>转移矩阵</code>。<br></p>
</div>
<div class="paragraph">
<p>接下来我们要介绍的 <code>MCMC</code> 算法是 <code>Metropolis</code> 算法的一个改进变种，即常用的 <code>Metropolis Hastings</code> 算法。由上节的例子和定理我们看到了，马氏链的收敛性质主要是由转移矩阵 <code>P</code> 决定的，
所以基于马氏链做采样的关键问题是如何构造转移矩阵 <code>P</code> ，使得平稳分布刚好是我们想要的分布\(p(x)\)。如何做到这一点呢？我们主要用到下面的定理。<br></p>
</div>
<div class="paragraph">
<p><strong>定理02(细致平稳条件)</strong> 如果非周期马氏链的转移矩阵 <code>P</code> 和分布\(\pi(x)\)满足：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\pi(x)P_{ij} = \pi_{j}P_{j} \hspace{1cm} for \hspace{0.1cm} all \hspace{0.1cm} i, j \hspace{2cm} (5.1)\]
</div>
</div>
<div class="paragraph">
<p>则\(\pi_{x}\)是马氏链的平稳分布，上式被称为 <code>细致平稳条件(detail balance condition)</code> 。证明也非常简洁:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\sum_{i=0}^{\infty }\pi(i)P_{ij}=\sum_{i=0}^{\infty }\pi(j)P_{ji}=\pi(j)\sum_{i=0}^{\infty }P_{ji}=\pi(j) \\
\Rightarrow \pi P=\pi \hspace{3cm}\]
</div>
</div>
<div class="paragraph">
<p>假设我们已经有一个转移矩阵为 <code>Q</code> 马氏链( \(p(i,j)\)表示从状态\(i\)转移为状态\(j\)的概率)，显然通常情况下<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(i)q(i,j)\neq p(j)q(j,i)\]
</div>
</div>
<div class="paragraph">
<p>也就是细致平稳条件不成立，所以\(p(x)\)不太可能是这个马氏链的平稳分布。我们可否对马氏链做一个改造，使得细致平稳条件成立呢？譬如我们引入\(\alpha(i,j)\)，我们希望：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(i)q(i,j)\alpha (i,j)= p(j)q(j,i)\alpha (j,i) \hspace{2cm} (5.3)\]
</div>
</div>
<div class="paragraph">
<p>取什么样的\(\alpha(i,j)\)以上等式能成立呢？最简单的，按照对称性，我们可以取：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\alpha (i,j)=p(j)q(j,i) \hspace{3cm} \alpha (j,i)=p(i)q(i,j)\]
</div>
</div>
<div class="paragraph">
<p>于是公式(5.3)就成立了，所以有<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\overset{p(i)\underbrace{q(i,j)\alpha (i,j)}= }{Q^{'}(i,j)}\overset{p(j)\underbrace{q(j,i)\alpha (j,i)} }{Q^{'}(j,i)} \hspace{2cm} (5.4)\]
</div>
</div>
<div class="paragraph">
<p>于是我们就把原来具有转移矩阵\(Q\)的一个普通的马氏链，改造成了具有转移矩阵\(Q^{'}\)的马氏链，而 \(Q^{'}\) 恰好满足细致平稳条件，由此马氏链\(Q^{'}\)的平稳分布就是\(p(x)\)！<br></p>
</div>
<div class="paragraph">
<p>暂停一下，让我们来思考两个问题<br></p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>我们为何要找满足细致平稳条件的转移矩阵？</p>
</li>
<li>
<p>p(x)我们并没有改变，为何改变转移矩阵后就成了平稳分布了？</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>第二个问题比较容易，因为平稳分布就是相对于转移矩阵的，不管p(x)初始状态是什么，转移矩阵都是使得p(x)最终收敛到平稳分布。 <code>那为何p(x)就是平稳分布呢？</code><br></p>
</div>
<div class="paragraph">
<p>这个问题和第一个问题是等价的，首先我们的目的是为了找到转移矩阵使得不论初始状态为何，都可以最后收敛到稳定分布。首先，这个转移矩阵不好找；其次，这个转移矩阵不止一个。
因此我们只需要找到一个就可以了，我们寻找的思路是从平稳分布和转移矩阵的确定关系出发，发现所有的平稳分布和转移矩阵都满足细致平稳条件，因此我们只要找到满足细致平稳条件的分布和转移矩阵，
那么这个分布就是平稳分布，因为满足平稳分布的定义。<br></p>
</div>
<div class="paragraph">
<p>在改造\(Q\)的过程中，我们引入了\(\alpha(i,j)\)称为接受率，物理意义可以理解为在原来的马氏链上，从状态\(i\)以\(q(i,j)\)的概率跳转到状态\(j\)的时候，我们以\(\alpha(i,j)\)的概率接受这个转移，
于是得到新的马氏链\(Q^{'}\)的转移矩阵为\(q(i,j)\alpha(i,j)\)。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/markov_transfer_accept.png" alt="markov transfer accept" width="50%" height="55%">
</div>
<div class="title">图 12. 马氏链转移和接受概率</div>
</div>
<div class="paragraph">
<p>把以上的过程整理一下，我们就可以得到如下的用于采样的概率分布\(p(x)\)的算法：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/hastings_algorithm.png" alt="hastings algorithm" width="50%" height="55%">
</div>
<div class="title">图 13. MCMC采样算法</div>
</div>
<div class="paragraph">
<p>以上过程不仅适应于离散的情形，对于分布是连续的，以上算法仍然有效。以上的 <code>MCMC</code> 算法已经能很漂亮的工作了，不过它有一个小问题：马氏链在状态转移过程中的接受率\(\alpha(i,j)\)可能偏小，
这样采样过程中马氏链容易原地踏步，拒绝大量的跳转，这使得马氏链 <code>遍历所有的状态空间</code> 要花费太长的时间，收敛到平稳分布\(p(x)\)的速度太慢，有没有办法提高接受率呢？<br></p>
</div>
<div class="paragraph">
<p>可以假设我们的接受率\(\alpha(i,j)=0.1\),\(\alpha(j,i)=0.2\)，此时满足细致平稳条件，于是：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(i)q(i,j) \times 0.1 = p(j)q(j,i) \times 0.2\]
</div>
</div>
<div class="paragraph">
<p>上式两边扩大五倍，可以改写为：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(i)q(i,j) \times 0.5 = p(j)q(j,i) \times 1\]
</div>
</div>
<div class="paragraph">
<p>看，我们提高了接受率，而细致平稳条件并没有打破！这启发我们可以把细致平稳条件中的接受率同比例放大，使得两个数中的最大一个数放大到1，这样我们就提高了采样的跳转接受率，所以我们可以取<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\alpha (i,j)=min\left \{ \frac{p(j)q(j,i)}{p(j)q(i,j)},1 \right \}\]
</div>
</div>
<div class="paragraph">
<p>经过如上改动，我们就得到了最常见的 <code>Metropolis-Hastings</code> 算法，算法伪代码如下：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/metropolis-hastings.png" alt="metropolis hastings" width="50%" height="55%">
</div>
<div class="title">图 14. Metropolis-Hastings算法</div>
</div>
<div class="paragraph">
<p><strong>至此，我们已经得到了一个解决方案，即不论我们给文档中的每个词初始化哪个主题编号，只要找到转移矩阵，我们都可以在迭代有限步后收敛到主题和词的稳定分布</strong>，伟大的 <code>Metropolis-Hastings</code> 算法就是我们的救世主，
而且它还告诉我们如何选取这样的转移矩阵，只是有一点瑕疵，这个转移矩阵虽然进行了优化，接受率仍然是个概率值，如果接受率为100%，那该多好，算法的收敛速度将达到最快。<br></p>
</div>
<div class="paragraph">
<p>真的还可以再优化吗？ 科学家们为了发表论文可真没闲着，因为100%接受率的采样方法真的找到了，掌声欢迎 <code>Gibbs Sampling</code> 算法华丽登场 。<br></p>
</div>
</div>
<div class="sect4">
<h5 id="_gibbs_sampling_算法">5.2.3.3. Gibbs Sampling 算法</h5>
<div class="paragraph">
<p><code>Metropolis-Hastings</code> 算法由于存在接受率的问题，因此对于高维空间的采样效率并不高，能否找到一个转移矩阵\(Q\)使得接受率\(alpha=1\)呢？<br></p>
</div>
<div class="paragraph">
<p>首先看下二维情况，假设有一个概率分布\(p(x,y)\)，考察\(x\)坐标相同的两个点\(A(x_{1}, y_{1})\)，\(B(x_{2}, y_{1})\)，我们发现：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(x_{1}, y_{1})p(y_{2}|x_{1}) = p(x_{1})p(y_{1}|x_{1})p(y_{2}|x_{1}) \\
p(x_{1}, y_{2})p(y_{1}|x_{1}) = p(x_{1})p(y_{2}|x_{1})p(y_{1}|x_{1})\]
</div>
</div>
<div class="paragraph">
<p>上面两公式相等，所以我们得到<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(x_{1}, y_{1})p(y_{2}|x_{1}) = p(x_{1}, y_{2})p(y_{1}|x_{1}) \hspace{2cm} (5.4)\]
</div>
</div>
<div class="paragraph">
<p>即<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(A)p(y_{2}|x_{1}) = p(B)p(y_{1}|x_{1})\]
</div>
</div>
<div class="paragraph">
<p>基于以上等式，我们发现在\(x=x_{1}\)这条平行于\(y\)轴的直线上，如果使用条件分布\(p(y|x_{1})\)作为任何两个点之间的转移概率，那么任意两个点之间的转移满足细致平稳条件。
同样的，如果我们在\(y=y_{1}\)这条直线上任意取两个点\(A(x_{1}, y_{1})\)，\(C(x_{2}, y_{1})\)，也有如下等式：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(A)p(x_{2}|y_{1}) = p(C)p(x_{1}|y_{1})\]
</div>
</div>
<div class="paragraph">
<p>于是我们可以如下构造平面上任意两点的之间的转移概率矩阵\(Q\)：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Q(A \rightarrow B) = p(y_{B} | x_{1}) \hspace{3cm} if \hspace{0.5cm} x_{A}=x_{B}=x_{1} \\
Q(A \rightarrow C) = p(y_{C} | x_{1}) \hspace{3cm} if \hspace{0.5cm} x_{A}=x_{C}=x_{1} \\
Q(A \rightarrow D) = 0 \hspace{5cm} other\]
</div>
</div>
<div class="paragraph">
<p>有了如上的转移矩阵\(Q\)，我们很容易验证对于平面上的任意两点\(X\),\(Y\)，满足细致平稳条件：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(X)Q(X \rightarrow Y) = p(Y)Q(Y \rightarrow X)\]
</div>
</div>
<div class="paragraph">
<p>于是，这个二维空间的马氏链收敛到平稳分布\(p(x,y)\)，而这个算法就是 <code>Gibbs Sampling</code> 算法，由物理学家 <code>Gibbs</code> 首次提出。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/gibbs_algorithm.png" alt="gibbs algorithm" width="50%" height="55%">
</div>
<div class="title">图 15. Gibbs Sampling算法</div>
</div>
<div class="paragraph">
<p>如图所示，马氏链的转移只是轮换的沿着坐标轴做转移，于是得到样本\((x_{0}, y_{0}), (x_{0},y_{1}), (x_{1}, y_{1}), (x_{1}, y_{2}), &#8230;&#8203;\)，
马氏链收敛后，最终得到的样本就是\(p(x,y)\)的样本。补充说明下，教科书上的 <code>Gibbs Sampling</code> 算法大都是坐标轮转算法，但其实这不是强制要求的。最一般的情况是，
在任意\(t\)时刻，可以在\(x\)轴和\(y\)轴之间随机的选一个坐标轴，然后按照条件概率做转移，马氏链也是一样收敛的。轮换两个坐标只是一种方便的形式。<br></p>
</div>
<div class="paragraph">
<p>以上的过程，我们很容易推广到高维的情况，对于\(n\)维空间，概率分布\(p(x_{1},x_{2},x_{3},&#8230;&#8203;,x_{n})\)可以如下定义转移矩阵：<br></p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>如果当前状态为\(x_{1}, x_{2}, &#8230;&#8203;, x_{n}\)，马氏链转移的过程中，只能沿着坐标轴做转移。沿着\(x_{i}\)这根坐标轴做转移的时候，转移概率由条件概率\(p(x_{i}|x_{1},&#8230;&#8203;,x_{i-1},x_{i+1},..,x_{n} )\)定义；</p>
</li>
<li>
<p>其他无法沿着单根坐标轴进行的跳转，转移概率都设置为0。</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>于是，我们可以把二维的 <code>Gibbs Sampling</code> 算法从采样二维的\(p(x,y)\)推广到采样\(n\)维的\(p(x_{1}, x_{2}, &#8230;&#8203;, x_{n})\)。以上算法收敛后，得到的就是概率分布\(p(x_{1}, x_{2}, &#8230;&#8203;, x_{n})\)的样本，
在通常的算法实现中，坐标轮转都是一个确定性的过程，也就是说在给定时刻\(t\)，在一根固定的坐标轴上转移的概率是1. 高维算法的伪代码如下：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/nd_gibbs.png" alt="nd gibbs" width="50%" height="55%">
</div>
<div class="title">图 16. n维Gibbs Sampling算法</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_第三次思考">5.2.4. 第三次思考</h4>
<div class="paragraph">
<p>看到这里希望大家的思路还是清晰的，让我们再一起思考下：<br></p>
</div>
<div class="exampleblock">
<div class="title">示例 6. 脑洞大开</div>
<div class="content">
<div class="ulist square">
<ul class="square">
<li>
<p>我们已经知道通过坐标轮转的方式，即 <code>Gibbs Sampling</code> 算法，可以让我们开始时任意指定文章中任意词对应的主题(即主题对应的词分布),经过迭代都可以收敛到平稳状态，也就是得到我们想要的topic-word分布</p>
</li>
<li>
<p>那么，具体到我们的问题中，该如何应用 <code>坐标轮转大法</code>, 是几维空间，坐标轴是什么？如何做到只沿着一个坐标轴轮转？</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>在自然语言处理中，我们经常将词映射到高维空间，因此这里一个很自然的想法就是：同样将词作为高维空间的维度，我们迭代第\(i\)个词时，坐标轮转法就意味着：在迭代过程中，固定当前词不变，
考虑条件概率分布\(p(z_{i}=k|\vec{z}_{\neg i},\vec{w} )\),这个条件概率的含义是在已知除了第\(i\)个词意外所有词的主题分布和可观察到的所有词的前提下，第\(i\)个词等于第\(k\)个主题的概率，公式中有一个符号：\(\neg \) 表示逻辑关系 <code>非</code>。<br></p>
</div>
<div class="paragraph">
<p><strong>理解上面的思维转换过程是非常重要的！！！</strong> 其实大家只要对照前面讲的Gibbs采样的样本结果还是很容易理解的，每次更新一个坐标，保持其他坐标轴值不变，也就是每次只更新一个词的主题编号，条件是已知其他词的主题编号。<br></p>
</div>
<div class="paragraph">
<p>如果知道了条件概率分布\(p(z_{i}=k|\vec{z}_{\neg i},\vec{w} )\)，只需要对每个词按照概率抽样对应的主题编号就可以了。问题终于抽象为了一个数学问题，即求解公式(5.5)的值：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(z_{i}=k|\vec{z}_{\neg i},\vec{w}) \hspace{3cm} (5.5)\]
</div>
</div>
<div class="paragraph">
<p>似乎没什么思路，一个大胆的想法就是既然条件概率不好求，是不是可以求出联合概率分布，然后利用贝叶斯公式间接求解？ 这个方法很老套，但不妨试试。既然要计算联合概率分布，首先想到的是 <code>概率图模型</code>，把这些变量之间的关系都画出来,瞬间写出联合概率分布.<br></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_文本建模">5.3. 文本建模</h3>
<div class="paragraph">
<p>关于上节的结论，我还想再多说一句，其实大家从很自然角度去想公式(5.5)，你会发现，它其实就是在说：文本中的某个词的主题分布，是由它周围(或者说文档中其他)词共同决定的，这个很好理解，毕竟文档中所有的词都是在描述同样的事情。既然这个概率公式的解释如此自然，
我们就从文本自然产生的过程出发，来考虑其联合概率分布。<br></p>
</div>
<div class="sect3">
<h4 id="_文档是如何产生的">5.3.1. 文档是如何产生的</h4>
<div class="paragraph">
<p>我们可以看看日常生活中的人是如何构思文章的。如果我们要写一篇文章，往往是要先确定写哪几个主题。譬如构思一篇自然语言处理相关的文章，可能40%会谈论语言学、30%谈论概率统计、20%谈论计算机、还有10%谈论其他主题：<br></p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>说到语言学，我们容易想到的词汇：语法、句子、乔姆斯基、主语&#8230;&#8203;</p>
</li>
<li>
<p>谈到概率统计，我们容易想到一下词：概率、模型、均值、方差、证明、独立、马尔可夫链&#8230;&#8203;</p>
</li>
<li>
<p>谈论计算机，我们容易想到的词是：内存、硬盘、编程、二进制、对象、算法、复杂度&#8230;&#8203;</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>我们之所以立刻想到这些词，是因为这些词在对应的主题下出现的概率很高。我们可以很自然的看到，一篇文章通常是由多个主题构成，而每一个主题大概可以用与该主题相关的频率高的一些词来描述。<br></p>
</div>
<div class="paragraph">
<p>以上的直观想法由 <code>Hoffmn</code> 于1999年给出的 <code>PLSA(Probabilistic Latent Semantic Analysis)</code> 模型中首先进行了明确的数学化，<code>Hoffmn</code> 认为一篇文档可以有多个主题\(Topic\)混合而成，而每个主题都是词汇上的概率分布，
文章中的每个词都是由一个固定的\(Topic\)生成的。<br></p>
</div>
</div>
<div class="sect3">
<h4 id="_plsa模型">5.3.2. PLSA模型</h4>
<div class="paragraph">
<p>统计学被人们描述为猜测上帝的游戏，人类产生的说有的预料文本我们都可以看成是一个伟大的上帝在天堂中抛掷骰子生成的，我们观察到的都是上帝玩这个游戏的结果，所以在文本建模中，我们希望猜测出上帝是如何玩这个游戏的，
具体一点，最核心的两个问题是：<br></p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>上帝都有什么样的骰子</p>
</li>
<li>
<p>上帝是如何抛掷这些骰子的</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p><code>Hoffmn</code> 认为上帝是按照如下的游戏规则来生成文本的。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/plsa_holy.png" alt="plsa holy" width="50%" height="55%">
</div>
<div class="title">图 17. 游戏：PLSA主题模型</div>
</div>
<div class="paragraph">
<p>以上 <code>PLSA</code> 模型的文档生成的过程可以图形化的表示为：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/plsa_doc_gen.png" alt="plsa doc gen" width="50%" height="55%">
</div>
<div class="title">图 18. PLSA模型的文档生成过程</div>
</div>
<div class="paragraph">
<p>我们可以发现，按照上面的游戏规则，文档和文档之间是独立可交换的，同一个文档内的词也是独立可交换的，这是一个典型的词袋 <code>bag-of-words</code> 模型。
游戏中的\(K\)个 <code>topic-word</code> 骰子，我们可以记为\(\vec{\varphi_{1}}, &#8230;&#8203; \vec{\varphi_{K}}\),
对于包含\(M\)篇文档的预料 \( C=(d_{1}, d_{2},&#8230;&#8203;, d_{M}) \)中的每篇文档\(d_{m}\)，都会有一个特定的 <code>doc-topic</code> 骰子\(\vec{\theta_{m}}\),
所以对应的骰子记为\(\vec{\varphi_{1}}, &#8230;&#8203; \vec{\varphi_{M}}\),为了方便，我们假设每个词\(w\)都是一个编号，对应到 <code>topic-word</code> 骰子的面。
于是在 <code>PLSA</code> 模型中，第\(m\)篇文档\(d_{m}\)中的每个词的生成概率为：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(w|d_{m})=\sum_{z=1}^{K}p(w|z)p(z|d_{m})=\sum_{z=1}^{K}\varphi _{zw}\theta _{mz} \hspace{2cm} (5.6)\]
</div>
</div>
<div class="paragraph">
<p>所以整篇文章的生成概率为:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(\vec{w}|d_{m})=\prod_{i=1}^{n}\sum_{z=1}^{K}p(w_{i}|z)p(z|d_{m})=\prod_{i=1}^{n}\sum_{z=1}^{K}\varphi _{zw_{i}}\theta _{mz} \hspace{1cm} (5.7)\]
</div>
</div>
<div class="paragraph">
<p>由于文本之间相互独立，我们也容易写出整个预料的生成概率。求解 <code>PLSA</code> 这个主题模型的过程中，模型参数容易求解，可以使用著名的 <code>EM</code> 算法进行求得局部最优解，由于该模型的解并不是本章节的重点，
有兴趣的同学可以参考 <code>Hoffmn</code> 的原始论文，此处略。</p>
</div>
</div>
<div class="sect3">
<h4 id="_lda模型">5.3.3. LDA模型</h4>
<div class="paragraph">
<p>对于 <code>PLSA</code> 模型，贝叶斯学派显然是有意见的，<code>doc-topic</code> 骰子\(\vec{\theta_{m}}\)和 <code>topic-word</code> 骰子\(\vec{\varphi _{k}}\)都是模型中的参数，参数都是随机变量，怎么能没有先验分布呢？
于是对于 <code>PLSA</code> 模型的贝叶斯改造，我们可以在如下两个骰子参数前加上先验概率。<br></p>
</div>
<div class="paragraph">
<p>应该增加什么样的先验分布呢？ 假定先验概率为\(X\)分布，在似然函数为多项分布的情况下，观察到一定的数据后，得到的后验概率\(Y\),然后后验概率\(Y\)会作为下一次观察前的先验概率，
因此我们希望先验概率分布和后验概率分布相同，即\(X=Y\)，在似然函数为多项分布的情况下，什么样的分布满足先验分布和后验分布相同呢？</p>
</div>
<div class="sect4">
<h5 id="_二项分布">5.3.3.1. 二项分布</h5>
<div class="paragraph">
<p>在数学上定义，在指定似然函数下，先验分布和后验分布相同时，先验分布就叫做似然函数的共轭分布，一个容易让人误解的概念就是：共轭先验是先验概率相对于似然函数而言的。
那么究竟什么分布和多项分布共轭？<br></p>
</div>
<div class="paragraph">
<p>问题简化下，我们知道多项分布是二项分布在高维度上的推广，我们先研究二项分布的共轭先验分布！<br></p>
</div>
<div class="paragraph">
<p>假定上帝和我们做一个游戏：游戏的规则很简单，上帝由一个魔盒，上面有个按钮，你每按一下按钮，就会均匀的输出一个[0~1]之间的随机数，上帝现在按了10下，手上有10个数，
你猜第7大的数是什么。你该如何猜呢？<br></p>
</div>
<div class="paragraph">
<p>从数学角度描述这个游戏如下：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/game_one.png" alt="game one" width="50%" height="55%">
</div>
<div class="title">图 19. 游戏1</div>
</div>
<div class="paragraph">
<p>对于上面的游戏而言\(n=10, k=7\)，如果我们能求出\(X_{(7)}\)的分布的概率密度，那么用概率密度的极值点去做猜测就是最好的策略。那么对于一般的情形，\(X_{(k)}\)的分布是什么呢？
我们尝试计算\(X_{(k)}\)在一个区间\([x, x+\Delta x]\)的概率，也就是如下的概率值：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(x\leqslant X_{(k)}\leqslant x+\Delta x) = ?\]
</div>
</div>
<div class="paragraph">
<p>把[0,1]区间分成三段\([0,x),[x,x+\Delta x], (x+\Delta x, 1]\)，我们先考虑简单的情形，假定n个数字中只有一个落在区间\([x,x+\Delta x]\)内，则因为该区间是第\(k\)大的，
则\([0,x)\)应该有\(k-1\)个数，\((x+ \Delta x,1]\)之间应该有\(n-k\)个数，我们将符合上述要求的事件记为\(E\)：<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/thing_E.png" alt="thing E" width="50%" height="55%">
</div>
<div class="title">图 20. 事件E</div>
</div>
<div class="paragraph">
<p>则有：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
P(E) &amp; = \prod_{i=1}^{n}P(X_{i}) \\
&amp; = x^{k-1}(1-x-\Delta x)^{n-k} \Delta x \\
&amp; = x^{k-1} (1-x)^{n-k} \Delta x + o(\Delta x)
\end{align}</p>
</div>
<div class="paragraph">
<p>其中\(o(\Delta x)\)表示\(\Delta x\)的高阶无穷小。显然，由于不同的排列组合，即\(n\)个数中有一个数落在\([x,x+\Delta x]\)区间的有\(n\)种取法，
余下\(n-1\)个数落在\([0,x)\)的有\(C_{n-1}^{k-1}\)种组合，所以和事件\(E\)等价的事件一共有 \(nC_{n-1}^{k-1}\)个。<br></p>
</div>
<div class="paragraph">
<p>刚才我们假定了在\([x,x+\Delta x]\)区间只有一个数的情况，再考虑稍微复杂点的情况，假设有两个数落在了区间\([x,x+\Delta x]\),事件记为\(E^{'}\),则有<br></p>
</div>
<div class="stemblock">
<div class="content">
\[P(E^{'})=x^{k-2}(1-x-\Delta x)^{n-k}(\Delta x)^{2}=o(\Delta x)\]
</div>
</div>
<div class="paragraph">
<p>从以上的分析可以很容易看出，只要落在\([x,x+\Delta x]\)内的数字超过一个，则对应事件的概率就是\(\Delta x\)，于是：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
P(x\leq X_{(k)}\leq x+\Delta x) &amp; = nC_{n-1}^{k-1}P(E)+o(\Delta x) \\
&amp; = nC_{n-1}^{k-1} x^{k-1} (1-x)^{n-k} \Delta x + o(\Delta x)
\end{align}</p>
</div>
<div class="paragraph">
<p>所以，可以得到\(X_{(k)}\)的概率密度函数为：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
f(x) &amp; = \lim_{\Delta x \to0 }\frac{P(x\leq X_{(k)} \leq x+\Delta x)}{\Delta x} \\
&amp; = nC_{n-1}^{k-1} x^{k-1} (1-x)^{n-k} \\
&amp; = \frac{n!}{(k-1)!(n-k)!}x^{k-1} (1-x)^{n-k} \hspace{0.5cm} x\in [0,1]
\end{align}</p>
</div>
<div class="paragraph">
<p>这面的公式中有阶乘，因此可以用 <code>Gamma</code> 函数来表示，我们可以把\(f(x)\)表达为：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[f(x) = \frac{\Gamma (n+1)}{\Gamma (k) \Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}  \hspace{2cm} (5.8)\]
</div>
</div>
<div class="paragraph">
<p>考虑到可能有同学不了解 <code>Gamma</code> 函数，我们接下来会补充介绍一下，了解的同学可以跳过下面一个小节。</p>
</div>
</div>
<div class="sect4">
<h5 id="_神奇的_gamma_函数">5.3.3.2. 神奇的 <code>Gamma</code> 函数</h5>
<div class="paragraph">
<p>学高数的时候，我们都学过如下一个长相有点奇特的 <code>Gamma</code> 函数<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\Gamma (x) = \int_{0}^{\infty } t^{x-1} e^{-t} dt\]
</div>
</div>
<div class="paragraph">
<p>通过分布积分，我们可以求得<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\Gamma (x) &amp; = \int_{0}^{\infty } t^{x-1} e^{-t} dt \\
&amp; =\frac{1}{x} \int_{0}^{\infty} e^{-t} d t^{x} \\
&amp; =\frac{1}{x} [ (e^{-t} t^{x}) \mid_{0}^{\infty} -\int_{0}^{\infty} t^{x} d e^{-t} ] \\
&amp; = \frac{1}{x}[0 - e^{-t} (-1) \int_{0}^{\infty} t^{x} e^{-t} dt] \\
&amp; = \frac{1}{x} \int_{0}^{\infty} t^{x} e^{-t} dt \\
&amp; = \frac{1}{x} \Gamma(x+1)
\end{align}</p>
</div>
<div class="paragraph">
<p>所以推导出了如下的递归性质<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\Gamma(x+1) = x \Gamma(x)\]
</div>
</div>
<div class="paragraph">
<p>看到上式，你一定会想到 <code>斐波那契数列</code> ，它可以表示为数的阶乘的形式！ 于是很容易证明， <code>Gamma</code> 函数可以当成是阶乘在实数集上的扩展，
具有如下性质：<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\Gamma(n) = (n-1)!\]
</div>
</div>
</div>
<div class="sect4">
<h5 id="_beta_函数">5.3.3.3. <code>Beta</code> 函数</h5>
<div class="paragraph">
<p>让我们再回到公式(5.8)，我们令\(\alpha=k, \beta=n-k+1\)，于是我们得到<br></p>
</div>
<div class="stemblock">
<div class="content">
\[f(x) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha -1} (1-x)^{\beta - 1} \hspace{1cm} (5.9)\]
</div>
</div>
<div class="paragraph">
<p>这个函数，就是一般意义上的 <code>Beta</code> 分布，上面的例子中\(n=10, k=7\)，所以我们按照如下密度分布的峰值去猜测才是最有把握的。<br></p>
</div>
<div class="stemblock">
<div class="content">
\[f(x) = \frac{10!}{6!3!} x^{6} (1-x)^{3} \hspace{0.5cm} x\in [0,1]\]
</div>
</div>
<div class="paragraph">
<p>然而即便如此，我们能做到一次猜中的概率仍然很低，因为我们掌握到的信息量是在是太少了，如果上帝仁慈了点，告诉了我们一些信息，
那么我的对 <code>手头上第7大的数是多少</code> 这件事会得到怎样的后验概率分布呢？ 首先已知的是：在没有任何知识的情况下，这个概率分布为 <code>Beta</code> 分布.<br></p>
</div>
<div class="stemblock">
<div class="content">
\[先验分布 + 数据的知识 = 后验分布(?)\]
</div>
</div>
<div class="paragraph">
<p>以上是贝叶斯分析过程的简单直观的表述，假定上帝给我们的信息为：<strong>上帝按了5下这个机器，你就得到了5个[0,1]之间的随机数，然后上帝告诉你这5个数中的每个数，以及和第7个数相比，谁大谁小，然后再让你猜上帝手头上第7大的数是多少。</strong><br></p>
</div>
<div class="paragraph">
<p>上帝的第二个游戏，数学上形式化一下，就是<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/game2.png" alt="game2" width="50%" height="55%">
</div>
<div class="title">图 21. 游戏2</div>
</div>
<div class="paragraph">
<p>我看网上很多人质疑 <code>Richjin</code> 给出的这几个例子，我觉得质疑者多半是没有看懂这些例子背后的含义。让我们仔细阅读上面的游戏规则，
实际上这个就是一个典型的探索先验分布、似然函数以及后验分布关系的例子，千万别忘了走到这一步我们的目的是什么： <code>在似然函数为多项分布的时候，什么样的分布满足先验分布和后验分布共轭，为了简化问题，我们将多项分布退化到二项分布来研究这个问题。</code><br></p>
</div>
<div class="paragraph">
<p>好，上面又啰嗦的梳理了下我们的思路，让我们继续考虑第2个游戏。由于\(p=X_{(k)}\)在\(X_{1}, X_{2}, &#8230;&#8203;, X_{n}\)中是第\(k\)大的数，利用\(Y_{i}\)的信息，
我们很容易得到\(p\)在\(X_{1}, X_{2}, &#8230;&#8203;, X_{n}, Y_{1}, Y_{2},&#8230;&#8203;, Y_{m}\) 这\((m+n)\)个独立同分布的随机变量中是第\(k+m_{1}\)个，于是按照第1个游戏的方式，我们可以得到\(p=X_{k}\)的概率密度函数是<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Beta(p|k+m_{1}, n-k+1+m_{2})\]
</div>
</div>
<div class="paragraph">
<p>讲到这里，我们把以上的过程再整理如下：</p>
</div>
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>\(p=X_{k}\)是我们猜测的参数，我们推导出\(p\)的分布为\(f(p)=Beta(p|k,n-k+1)\)称为\(p\)的先验分布；</p>
</li>
<li>
<p>数据\(Y_{i}\)中有\(m_{1}\)个比\(p\)大，\(m_{2}\)个比\(p\)小，\(Y_{i}\)相当于做了\(m\)次伯努利实验，所以\(m_{i}\)服从二项分布\(B(m,p)\);</p>
</li>
<li>
<p>在给定来自数据提供的\((m_{1}, m_{2})\)的知识后，\(p\)的后验分布变为了\(f(p|m_{1},m_{2})=Beta(p|k+m_{1}, n-k+1+m_{2})\)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>以上总结可以写成如下公式<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Beta(p|k,n-k+1) + BinomCount(m_{1}, m_{2}) = Beta(p|k+m_{1}, n-k+1+m_{2})\]
</div>
</div>
<div class="paragraph">
<p>其中\((m_{1}, m_{2})\)对应的是二项分布\(B(m_{1}+m_{2}, p )\)的计数，更为一般的，对于非负实数\(\alpha, \beta\)，我们有如下关系<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Beta(p|\alpha,\beta) = BinomCount(m_{1}, m_{2}) = Beta(p| \alpha+m_{1}, \beta+m+{2})\]
</div>
</div>
<div class="paragraph">
<p>这个式子描述的就是 <code>Beta-Binomial共轭</code>，此处共轭的意思就是：<strong><em>数据符合二项分布的时候，参数的先验分布和后验分布都能保持 <code>Beta</code> 分布的形式</em>。</strong><br></p>
</div>
<div class="paragraph">
<p>而我们从以上过程可以看到，<code>Beta</code> 分布的参数\(\alpha,\beta\)都可以理解为物理计数，这两个参数经常被称为伪计数\((pseudo-count)\)，基于以上逻辑，
我们也可以把 <code>Beta</code> 分布写成如下的公式来理解<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Beta(p|1,1) + BinomCount(\alpha - 1, \beta - 1) = Beta(p|\alpha , \beta) \hspace{1cm} (5.10)\]
</div>
</div>
<div class="paragraph">
<p>其中 <code>Beta(p|1,1)</code> 恰好就是均匀分布 <code>Uniform(0, 1)</code> 。<br></p>
</div>
<div class="paragraph">
<p>对于公式(5.10)，我们其实可以从贝叶斯的角度来进行推导和理解。假设有一个不均匀的硬币抛出正面的概率\(p\)，抛\(m\)次后出现正面和反面的次数分别为\(m_{1}, m_{2}\)，
那么按传统的频率学派观点，\(p\)的估计值应该为\(\hat{p}=\frac{m_{1}}{m_{1}+m_{2}}\),而从贝叶斯学派角度来看，开始对硬币的不均匀性一无所知，所以应该假设
\(p~Uniform(0,1)\)，于是有了二项分布的计数(m_{1}, m_{2})之后，按照贝叶斯公式计算\(p\)的后验概率<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
P(p|m_{1}, m_{2}) &amp; = \frac{P(p)\cdot P(m_{1}, m_{2}|p)}{P(m_{1}, m_{2})} \\
&amp; = \frac{1 \cdot P(m_{1}, m_{2}|p)}{\int_{0}^{1}P(m_{1}, m_{2} |t)dt} \\
&amp; = \frac{C_{m}^{m_{1}} p^{m_{1}} (1-p)^{m_{2}}}{ \int_{0}^{1} C_{m}^{m_{1} } t^{m_{1}} (1-t)^{m_{2}} dt}\\
&amp; = \frac{p^{m_{1}} (1-p)^{m_{2}}}{ \int_{0}^{1} t^{m_{1}} (1-t)^{m_{2}} dt}
\end{align}</p>
</div>
<div class="paragraph">
<p>计算得到的后验概率正好是\(Beta(p|m_{1} + 1, m_{2} + 1)\)。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/beta-alloc.png" alt="beta alloc" width="50%" height="55%">
</div>
<div class="title">图 22. 百变星君Beta分布</div>
</div>
<div class="paragraph">
<p><code>Beta</code> 分布的概率密度我们把它画成图，会发现它是百变星君，而均匀分布也是特殊的 <code>Beta</code> 分布，由于 <code>Beta</code> 分布能够拟合如此之多的形状，因此它在统计数据拟合和贝叶斯分析中被广泛使用。<br></p>
</div>
</div>
<div class="sect4">
<h5 id="_第四次思考">5.3.3.4. 第四次思考</h5>
<div class="paragraph">
<p>这一次大家不用脑洞大开，我们一起来思考下面几个问题。<br></p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">核心概念</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>如何更直观的理解伪计数，因为似乎后验概率相对于先验概率就是改了这个东西而已</p>
</li>
<li>
<p>如何直观理解似然函数</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>首先，举例说明什么是似然函数，以二项分布的最大似然估计为例，假设投硬币实验中，进行了\(N\)次独立试验，\(n\)次朝上，\(N-n\)次朝下，假设朝上的概率为\(p\)，请使用最大似然估计这个参数\(p\)。<br></p>
</div>
<div class="paragraph">
<p>在这个\(N\)次的独立试验中，结果是服从参数为\(p\)的二项分布，因此对参数的求解，可以使用最大似然函数来做点估计，使用对数似然函数作为目标函数<br></p>
</div>
<div class="stemblock">
<div class="content">
\[f(n|p) = log(p^{n} (1-p)^{N-n}) \\
h(p) = log(p^{n} (1-p)^{N-n}) \hspace{1cm} (5.11) \\
\frac{\partial h(p)}{\partial p} = \frac{n}{p} - \frac{N-n}{1-p} = 0 \Rightarrow p = \frac{n}{N}\]
</div>
</div>
<div class="paragraph">
<p>公式(5.11)是一个关于参数的函数，就是似然函数，令似然函数导数为0，求出的结果就是参数的点估计。<br></p>
</div>
<div class="paragraph">
<p>这种估计方式是典型的频率学派的思想，结果全部由观察到的数据决定，也种方法在观察数据较少时，往往会有很大问题。 比如我们就某学校门口，通过观察通过校门的男女生人数来估计学校里男女比例。
如果观察到2两个都是女生，按照上面的计算公式得到的结论就会是学校里女生占比100%，这显然是不对的，那我们就来思考这个问题。<br></p>
</div>
<div class="paragraph">
<p>假设观察到的男生和女生的个数分别为\(N_{B}=1\)和\(N_{G}=4\)，频率学派的计算公式就是<br></p>
</div>
<div class="stemblock">
<div class="content">
\[P_{B} = \frac{N_{B}}{N_{B}+N_{G}} = 0.2 \hspace{1cm}
P_{G} = \frac{N_{G}}{N_{B}+N_{G}} = 0.8\]
</div>
</div>
<div class="paragraph">
<p>如果修正这个公式，使得在观察学生的数量较少时仍然不会太离谱，我们假定男生和女生的比例为\(1:1\)，给出如下公式<br></p>
</div>
<div class="stemblock">
<div class="content">
\[P_{B} = \frac{N_{B}+5}{N_{B}+N_{G}+10} = 0.4 \hspace{1cm}
P_{G} = \frac{N_{G}+5}{N_{B}+N_{G}+10} = 0.6\]
</div>
</div>
<div class="paragraph">
<p>增加了一点先验的知识，使得结果在观察数据量不大的时候，不会太离谱，此时先验的知识会有比较高的权重，当观察的数据量很大时，先验的知识影响力就变得微乎其微。而公式中的5和10就是先验分布的 <code>伪计数</code>。<br></p>
</div>
<div class="paragraph">
<p>务必要理解这个 <code>伪计数</code> 的含义，我们这么做的目的是为了让参数的点估计结果在观察数据量很小的时候仍然可以得到一个不太离谱的数，相当于给要计算的参数增加了先验知识，这里假设参数的先验概率符合均匀分布.
当数据量较少时，其实就是证据不足时我们仍然相信我们的先验概率，只不过根据观察到的证据稍微修正先验知识。
<strong><em>贝叶斯理论的核心就是如何根据观察到的数据更新先验知识，参数真正的分布是什么，只有上帝知道，我们永远无法得知，我们只有通过不断地观察学习，来更新我们的认知，使得我们的认知更接近真理！</em></strong><br></p>
</div>
<div class="paragraph">
<p>下面再来学术性的分析下先验概率和后验概率的关系，投掷一个非均匀硬币，可以使用参数为\(\theta\)的伯努利模型，\(\theta\)为正面朝上的概率，那么结果\(x\)的分布形式为<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(x|\theta) = \theta^{x} \cdot (1-\theta)^{1-x} \hspace{1cm} x={0,1}\]
</div>
</div>
<div class="paragraph">
<p>这其实是一个关于参数\(\theta\)的函数，因此叫做似然函数。我们已经知道了两点分布和二项分布的共轭先验是 <code>Beta</code> 分布，它具有两个参数\(\alpha, \beta\)，<code>Beta</code> 分布的形式为<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(\theta |\alpha ,\beta ) = \left\{\begin{matrix}
\frac{1}{B(\alpha ,\beta )} \theta ^{\alpha -1} (1-\theta)^{\beta - 1}, \theta\in [0,1] \\
0 \hspace{3.2cm} ,other
\end{matrix}\right.\]
</div>
</div>
<div class="paragraph">
<p>有了先验概率和似然函数，使用贝叶斯公式计算下后验概率<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(\theta|x) &amp; = \frac{p(x|\theta) \cdot p(\theta)}{p(x)} \\
&amp; \propto p(x|\theta) \cdot p(\theta) \\
&amp; = (\theta^{x} \cdot (1-\theta)^{1-x}) \cdot (\frac{1}{B(\alpha ,\beta )} \theta ^{\alpha -1} (1-\theta)^{\beta - 1}) \\
&amp; \propto \theta^{x} \cdot (1-\theta)^{1-x} \cdot \theta^{\alpha-1} \cdot (1-\theta)^{\beta - 1} \\
&amp; = \theta^{(x+\alpha)-1} (1-\theta)^{(1-x+\beta) - 1}
\end{align}</p>
</div>
<div class="paragraph">
<p>我们发现后验概率的分布和先验概率的分布完全一样，即伯努利分布的共轭先验是 <code>Beta</code> 分布。<br></p>
</div>
<div class="paragraph">
<p>参数\(\alpha, \beta\)是决定参数\(theta\)的参数，常称为超参数。在后验概率的最终表达式中，参数\(\alpha, \beta\)和\(x\)一起作为参数\(\theta\)的指数&#8212;&#8203;后验概率的参数为\((x+\alpha, 1-x+\beta)\)。
而这个指数的实际意义是：投币过程中，正面朝上的次数。参数\(\alpha, \beta\)先验的给出了在没有任何试验的情况下，硬币朝上的概率分布，因此参数\(\alpha, \beta\)被称为 <code>伪计数</code> 。</p>
</div>
</div>
<div class="sect4">
<h5 id="_dirichlet_分布">5.3.3.5. <code>Dirichlet</code> 分布</h5>
<div class="paragraph">
<p>多项分布是二项分布在高维空间的推广，多项分布的先验分布也是 <code>Beta</code> 分布在高维空间的推广，它就是 <code>Dirichlet 分布</code>。<br></p>
</div>
<div class="paragraph">
<p>此时此刻你一定要🙏感谢上帝，我们终于绕回来了，毕竟我们在说 <code>LDA</code> 模型，毕竟那个 <code>D</code> 代表的就是 <code>Dirichlet</code> ，我们走了两万五千里长征终于切入正题了。关于 <code>Dirichlet 分布</code> 也可以通过 <code>Rickjin</code> 的 <code>LDA数学八卦</code> 的游戏示例推导出相关结论，
但本节我们就免去那些繁琐的推导，毕竟它是二项分布在高维空间的推广，那就以对比推广的形式给出结论。<br></p>
</div>
<div class="paragraph">
<p>首先，再看下 <code>Beta分布</code><br></p>
</div>
<div class="stemblock">
<div class="content">
\[f(x) = \left\{\begin{matrix}
\frac{1}{B(\alpha, \beta)} x^{\alpha-1} (1-x)^{\beta - 1} , x\in [0,1]\\
0 \hspace{3cm} ,other
\end{matrix}\right.\]
</div>
</div>
<div class="paragraph">
<p>其中<br></p>
</div>
<div class="stemblock">
<div class="content">
\[B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)}\]
</div>
</div>
<div class="paragraph">
<p>对应的 <code>Dirichlet</code> 分布为<br></p>
</div>
<div class="stemblock">
<div class="content">
\[f(\vec{p}|\vec{\alpha }) = \left\{\begin{matrix}
\frac{1}{\Delta(\vec{\alpha })} \prod_{k=1}^{K} p_{k}^{\alpha_{k}-1} ,P_{k} \in [0,1]\\
0 \hspace{2cm} ,other
\end{matrix}\right.\]
</div>
</div>
<div class="paragraph">
<p>简记为<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Dir(\vec{p}|\vec{\alpha } ) = \frac{1}{\Delta (\vec{\alpha })} \prod_{k=1}^{K} p_{k}^{\alpha_{k} - 1}\]
</div>
</div>
<div class="paragraph">
<p>其中<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\Delta(\vec{\alpha }) = \frac{\prod_{k=1}^{K} \Gamma (\alpha _{k})} {\Gamma (\sum_{k=1}^{K} \alpha _{k})}\]
</div>
</div>
<div class="paragraph">
<p>对 <code>Dirichlet</code> 分布求积分，我们还可以得到一个\(\Delta (\vec{\alpha})\)的积分形式<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
&amp; \int Dir(\vec{p} \mid \vec{\alpha})  =  1 \\
&amp; \Rightarrow \frac{1}{\Delta(\vec{\alpha})}\int p_{k}^{ \alpha^{k} - 1} dp_{k}= 1 \\
&amp; \Rightarrow \Delta(\vec{\alpha})= \int \prod_{k=1}^{K} p_{k}^{ \alpha^{k} -1} dp_{k} \hspace{1cm} (5.12)
\end{align}</p>
</div>
<div class="paragraph">
<p>有了后验概率，我们应该如何估计我们的参数呢？合理的方式是使用后验分布的极大值点，或者是参数在后验分布中的平均值，本文中，我们取平均值作为参数的估计值。 <code>抽样分布的数学期望等于总体参数的真值</code>.<br></p>
</div>
<div class="paragraph">
<p>让我们来看下 <code>Beta/Dirichlet 分布</code> 的数学期望到底是什么？ 如果\(p ~ Beta(t|\alpha, \beta)\)，则<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
E(p) &amp; = \int_{0}^{1} t \cdot Beta(t|\alpha, \beta) dt \\
&amp; = \int_{0}^{1} t \cdot \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} t^{\alpha - 1} (1-t)^{\beta - 1}dt \\
&amp; = \frac{ \Gamma(\alpha + \beta) }{ \Gamma(\alpha) \Gamma(\beta)} t^{\alpha} (1-t)^{\beta -1} dt \hspace{1cm}(5.13)
\end{align}</p>
</div>
<div class="paragraph">
<p>上式右边的积分对应概率分布\(Beta(t| \alpha + 1, \beta)\)，对于这个分布，我们有<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\int_{0}^{1}\frac{ \Gamma(\alpha + \beta + 1) }{ \Gamma(\alpha+1) \Gamma(\beta)} t^{\alpha} (1-t)^{\beta -1} dt = 1\]
</div>
</div>
<div class="paragraph">
<p>代入公式(5.12)，可以得到<br>
\begin{align}
E(p) &amp; = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} \cdot \frac{ \Gamma(\alpha+1) \Gamma(\beta) }{ \Gamma(\alpha + \beta + 1) }  \\
&amp; = \frac{ \Gamma(\alpha + \beta) }{ \Gamma(\alpha + \beta + 1) } \cdot \frac{ \Gamma(\alpha +1 ) }{\Gamma(\alpha)} \\
&amp; = \frac{\alpha}{\alpha + \beta}
\end{align}
这说明，对于 <code>Beta分布</code> 的随机变量，其均值可以用\(\frac{\alpha}{\alpha + \beta}\)来估计。同理， <code>Dirichlet分布</code> 也有类似的结论，如果\(\vec{p}\sim Dir(\vec{t} \mid\vec{\alpha })\),
同样可以证明<br></p>
</div>
<div class="stemblock">
<div class="content">
\[E(\vec{p}) = \left ( \frac{\alpha _{1}}{ \sum_{i=1}^{K} \alpha _{i} },  \frac{\alpha _{2}}{ \sum_{i=1}^{K} \alpha _{i} },...,\frac{\alpha _{K}}{ \sum_{i=1}^{K} \alpha _{i} } \right ) \hspace{1cm} (5.14)\]
</div>
</div>
<div class="paragraph">
<p>以上两个结论非常重要，因为我们在后面的 <code>LDA</code> 数学推导中需要使用这个结论。简单想下为何后面会遇到，因为我们的条件概率是由贝叶斯公式来求解的，贝叶斯公式中有积分的情况，而高维空间的积分是不好求解的，但是我们知道随机变量和概率密度的积分就是期望！
哇塞，感觉到一把利剑在手，有么有？ 不懂没关系，后面还会讲解。<br></p>
</div>
<div class="paragraph">
<p>差点忘了我们要干嘛，有了多项分布的先验分布是 <code>Dirichlet分布</code> ，我们的目的是求在已知似然函数为多项分布的情况后验概率分布的参数会怎么更新，仍然对比二项分布来看<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Beta(p \mid \alpha, \beta) + BinomCount(m_{1}, m_{2}) = Beta(p \mid \alpha + m_{1}, \beta + m_{2}) \hspace{1cm} (5.15)\]
</div>
</div>
<div class="paragraph">
<p>来对比看下多项分布<br></p>
</div>
<div class="stemblock">
<div class="content">
\[Dir(\vec{p} \mid \vec{k}) + MultCount(\vec{m}) = Dir(\vec{p} \mid \vec{k} + \vec{m}) \hspace{5.5cm} (5.16)\]
</div>
</div>
<div class="paragraph">
<p>公式(5.16)非常重要，其中\(k\)为伪计数，\(m\)为观察到的属于\(p_{k}\)的个数。</p>
</div>
</div>
<div class="sect4">
<h5 id="_lda_模型">5.3.3.6. <code>LDA</code> 模型</h5>
<div class="paragraph">
<p>已经接近终点了，让我们做下来思索下走过的路，有没有迷路呢？<br></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>我们从最开始了了解到如果想知道一篇文章的主题，那么如果知道每个词对应的主题就好了，也就是需要知道每个主题对应的词分布；</p>
</li>
<li>
<p>然后我们希望给每个词随机赋值一个主题编号，通过迭代的方式，最后可以收敛到主题对应的词分布，当然这个前提不管我们开始给每个词赋值了什么主题编号，都不影响最后的结果；</p>
</li>
<li>
<p>神奇的事情发生了，我们发现马氏链的平稳分布就具有这个特点，怎么才能结合上呢？</p>
</li>
<li>
<p>我们研究了马氏链，发现获得平稳分布的关键在于转换矩阵，转换矩阵可能有很多，我们只需要找到一个就可以了，于是我们取巧，根据对称性发现了 <code>Metropolis Hastings 算法</code>;</p>
</li>
<li>
<p><code>Metropolis Hastings 算法</code> 有个缺陷就是在状态转移时有接受率的限制，接受率较低的话会导致很难遍历完所有马氏链的状态，算法收敛速度很慢，有没有接受率为100%的算法呢；</p>
</li>
<li>
<p>神奇的事情又发生了，沿着坐标轴进行转移时，发现接受率100%，我们无意中发现了 <code>Gibbs Sampling 算法</code>,这个算法的核心是坐标轮转，如何结合到我们的问题场景下呢；</p>
</li>
<li>
<p>我们发现将文档看作一个由词构成的高维空间，在更新每个词的主题时，只需要依据其他词的主题分布，固定其他词不变，只改变当前词的做法就是坐标轮转大法；</p>
</li>
<li>
<p>我们发现这个更新规则是个条件概率，不好求解，于是我们想到了贝叶斯公式，想到了全概率公式来求解的方法，那就开始研究这些变量之间的关系，以便获取变量间的全概率公式；</p>
</li>
<li>
<p>通过将所有变量的关系进行梳理，我们发现本文的生成就是一个上帝掷骰子的过程，写文章时其实首先要确定主题，然后根据主题写选择词汇，这个主题是个隐变量；</p>
</li>
<li>
<p>文章到主题是多项分布，主题到词汇也是多项分布，多项分布的参数模型可以认为是上帝手里的骰子，而既然是参数，是变量，贝叶斯学派是不会放任它太随意了，让这两个多项分布的参数都来个先验分布吧；</p>
</li>
<li>
<p>选择什么样的先验分布合适呢？贝叶斯学派关注的是在观察数据的基础上，是如何更新先验知识的，换句话说，先验分布加上观察数据变成后验分布，其实又是下一次观察的先验分布，因此我们希望找到先验和后验分布类型相同的分布；</p>
</li>
<li>
<p>为了简化问题，我们避开了直接研究多项分布的先验分布问题，因为那是高维空间的，不好抽象和理解；我们从多项分布的特例，即二项分布出发；</p>
</li>
<li>
<p>通过和上帝玩了几个游戏，我们神奇的发现：二项分布的共轭先验分布 <code>Beta分布</code>，于是我们顺势就将二项分布推广到了高维空间，发现多项分布的共轭先验分布是 <code>Dirichlet分布</code>;</p>
</li>
<li>
<p>既然知道了先验分布是 <code>Dirichlet分布</code>，返回去求解条件概率时会用到全概率公式，既然有高维积分的形式，那就看看是不是这个分布的期望比较好求呢？</p>
</li>
<li>
<p>神奇的事情又又又发生了， <code>Beta分布</code> 和 <code>Dirichlet分布</code> 的期望值都是如此的简洁，不得不感慨 <code>数学之美啊</code> ！</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>上面重新梳理了下思路，让我们按照上面的思路继续解决问题，下面让我们用 <code>概率图模型</code> 表示 <code>LDA</code> 模型的物理过程，如图所示。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/lda-graph-model.png" alt="lda graph model" width="30%" height="35%">
</div>
<div class="title">图 23. LDA概率图模型表示</div>
</div>
<div class="paragraph">
<p>这个概率图模型可以分解为两个主要的物理过程：<br></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>\(\vec{\alpha }\rightarrow \vec{\theta }\rightarrow z_{m,n}\),这个过程表示生成第\(m\)篇文档的时候，先从第一个坛子中抽了一个 <code>doc-topic</code> 骰子\(\vec{\theta }\),然后投掷这个骰子生成了文档中第\(n\)个词的主题编号\(z_{m,n}\)；</p>
</li>
<li>
<p>\(\vec{\beta }\rightarrow \vec{\varphi_{k}} \rightarrow w_{m,n} \mid k=z_{m,n}\),这个过程表示用如下动作生成语料中第\(m\)篇文档的第\(n\)个词：在上帝手头的\(K\)个 <code>topic-word</code> 骰子\(\vec{\varphi_{k}}\)中，挑选编号为\(k=z_{m,n}\)的那个骰子进行投掷，然后生成单词\(w_{m,n}\)；</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>大家都说理解 <code>LDA</code> 最重要的就是理解上面这两个物理过程，其实这两个物理过程还是很好理解的。首先我们来看下文章中的主题和词的联合分布<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(\vec{w}, \vec{z} \mid \vec{\alpha }, \vec{\beta })  = p(\vec{w} \mid \vec{z}, \vec{\beta }) p(\vec{z} \mid \vec{\alpha }) \hspace{1cm} (5.17)\]
</div>
</div>
<div class="paragraph">
<p>公式第一项因子是给定主题采样词的过程；后面的因子计算，\(n_{z}^{(t)}\)表示词\(t\)被观察到分配给主题\(z\)的次数， \(n_{m}^{(k)}\)表示主题\(k\)分配给文档\(m\)的次数。<br></p>
</div>
<div class="paragraph">
<p>计算第一个因子如下<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(\vec{w} \mid \vec{z}, \vec{ \beta } ) &amp; = \int p(\vec{w} \mid \vec{z}, \Phi ) \cdot p(\Phi  \mid \vec{\beta }) d\Phi  \\
&amp; = \int \prod_{z=1}^{K} \cdot \frac{1}{\Delta(\vec{\beta})} \prod_{t=1}^{V} \varphi_{ z,t }^{n_{ z }^{(t)} + \beta_{t} -1} d\vec{\varphi _{z}} \\
&amp; = \hspace{0.2cm} (continue&#8230;&#8203;)
\end{align}</p>
</div>
<div class="paragraph">
<p>这一步推导依据公式(5.16)，后面的概率是由参数\(\beta\)控制的 <code>Dirichlet分布</code>，前面的概率是多项分布，多项分布乘以先验 <code>Dirichlet分布</code> 得到的结果仍然为 <code>Dirichlet分布</code>,
其中\(\Phi\)为词分布，\(\vec{\varphi_{z}}\)为主题为\(z\)的词分布，因为共有\(K\)个主题，所以对主题向量\(\vec{z}\)中的每个主题对应的词分布\(\vec{\varphi_{z}}\)求积分.
根据公式(5.16)后验 <code>Dirichlet分布</code> 可以表示为归一化因子和参数的乘积，其中参数的伪计数由原来的伪计数\(\beta_{t}-1\)基础上增加观察量\(n_{z}^{(t)}\) 表示词\(t\)被分配为主题\(z\)的次数。<br></p>
</div>
<div class="stemblock">
<div class="content">
\[=\prod_{z=1}^{K} \frac{\Delta (\vec{n_{z}} + \vec{\beta })}{\Delta (\vec{\beta })} \hspace{0.2cm} (5.18)\]
</div>
</div>
<div class="paragraph">
<p>上一步推导依据公式(5.12)，将积分转换为归一化因子即可,其中<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\vec{n_{z}}=\left \{ n_{z}^{t} \right \}_{t=1}^{V}\]
</div>
</div>
<div class="paragraph">
<p>同理，计算公式(5.17)联合分布的第二个因子<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(\vec{z} \mid \vec{\alpha }) &amp; = \int p(\vec{z} \mid \Theta ) p(\Theta \mid \vec{\alpha }) d\Theta  \\
&amp; = \int \prod_{m=1}^{M} \frac{1}{\Delta(\vec{\alpha})} \prod_{k=1}^{K} \theta_{ m,k }^{ n_{m}^{ (k) } +\alpha_{k}-1} d\vec{\theta_{m}} \\
&amp; = \prod_{m=1}^{M}\frac{ \Delta (\vec{n_{m}} + \vec{\alpha}) }{ \Delta(\vec{\alpha}) } \hspace{1cm}, \vec{n_{m}}=\left \{ n_{m}^{(k)} \right \}_{k=1}^{K} \hspace{1cm}(5.19)
\end{align}</p>
</div>
<div class="paragraph">
<p>主题分布和词分布的计算思路完全相同，不再赘述。<br></p>
</div>
<div class="paragraph">
<p>将公式(5.18)和(5.19)代入第\(i\)个词主题编号的概率公式得到：<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(z_{i} &amp; = k \mid \vec{z_{ \neg i }}, \vec{w}) = \frac{p( \vec{w}, \vec{z} )}{p( \vec{w}, \vec{z_{\neg i}} )} = \frac{p( \vec{w} \mid \vec{z} )}{p(\vec{w_{ \neg i} } \mid \vec{z_{\neg i}} ) p(w_{i})} \cdot \frac{p(\vec{z})}{p(\vec{z_{\neg i}})} \\
&amp; = \propto \frac{ \Delta (\vec{n_{z}} + \vec{\beta}) }{\Delta( \vec{n_{z, \neg i}} + \vec{\beta} )} \cdot \frac{ \Delta( \vec{n_{m}} + \vec{\alpha} ) }{ \Delta ( \vec{n_{m, \neg i}} + \vec{\alpha}) } \\
&amp; = \frac{\Gamma (n_{k}^{(t)} + \beta_{t}) \Gamma( \sum_{t=1}^{V} n_{k, \neg i}^{(t)} + \beta_{t}  )}{ \Gamma( n_{k, \neg i}^{(t)} + \beta_{t}) \Gamma( \sum_{t=1}^{V} n_{k}^{(t)} + \beta_{t}   )  } \cdot \frac{ \Gamma( n_{m}^{(k)} + \alpha_{k}) \Gamma(\sum_{k=1}^{K} n_{m, \neg i}^{(k)} + \alpha_{k}) }{ \Gamma(n_{m, \neg i}^{(k)} + \alpha_{k}) \Gamma(\sum_{k=1}^{K} n_{m}^{(k)} + \alpha_{k}) } \\
&amp; = \frac{n_{k, \neg i}^{(t)} + \beta_{t} }{\sum_{t=1}^{V} n_{k, \neg i}^{(t)} + \beta_{t}} \cdot \frac{ n_{m, \neg i}^{(k)} + \alpha_{k}}{ [\sum_{k=1}^{K} n_{m, \neg i}^{(k)} + \alpha_{k}]-1}  \hspace{1cm} (5.20)\\
&amp; \propto \frac{n_{k, \neg i}^{(t)} + \beta_{t} }{\sum_{t=1}^{V} n_{k, \neg i}^{(t)} + \beta_{t}} \cdot (n_{m, \neg i}^{(k)} + \alpha_{k})
\end{align}</p>
</div>
<div class="paragraph">
<p>上面是邹博老师的推导，感觉这么推导结果不是那么美啊，还是看看 <code>Rickjin</code> 的积分转为期望的推导<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
p(z_{i} = k \mid \vec{z_{\neg i }}, \vec{w}) &amp; \propto p(z_{i}=k, w_{i}=t \mid \vec{z_{ \neg i }, \vec{ w_{ \neg i } }}) \\
&amp; = \int p(z_{i} = k, w_{i} = t, \vec{\theta_{m}}, \vec{\varphi_{k}} \mid \vec{z_{ \neg i}}, \vec{w_{\neg i}}) d\vec{\theta_{m} }d\vec{\varphi_{k}} \\
&amp; = \int p(z_{i} = k \mid \vec{\theta_{m}}) p( \vec{\theta_{m}} \mid \vec{z_{ \neg i}}, \vec{ w_{\neg i} } ) \cdot p(w_{i} = t \mid \vec{\varphi_{k}} ) p(\vec{\varphi_{k}} \mid \vec{z_{\neg i}}, \vec{w_{\neg i}}) d\vec{\theta_{m}} d\vec{\varphi_{k}} \\
&amp; = \int p(z_{i} = k \mid \vec{\theta_{m}}) Dir(\vec{\theta_{m}} \mid \vec{n_{m, \neg i}} + \vec{\alpha})d\vec{\theta_{m}} \cdot p(w_{i} = t \mid \vec{\varphi_{k}}) Dir(\vec{\varphi_{k}} \mid \vec{n_{k, \neg i}} + \vec{\beta}) d\vec{\varphi_{k}} \\
&amp; = \int \theta_{mk} Dir(\vec{\theta_{m}} \mid \vec{n_{m, \neg i}} + \vec{\alpha})d\vec{\theta_{m}}  \cdot \int \varphi_{kt}Dir(\vec{\varphi_{k}} \mid \vec{n_{k, \neg i}} + \vec{\beta}) d\vec{\varphi_{k}} \\
&amp; = E(\theta_{mk}) \cdot E(\varphi_{kt}) \\
&amp; = \hat{\theta_{mk}} \cdot \hat{\varphi_{kt}} \\
&amp; = \frac{ n_{m, \neg i}^{(k)} + \alpha_{k}}{\sum_{k=1}^{K} n_{m, \neg i}^{(k)} + \alpha_{k}} \cdot \frac{n_{k, \neg i}^{(t)} + \beta_{t} }{\sum_{t=1}^{V} n_{k, \neg i}^{(t)} + \beta_{t}} \hspace{1cm} (5.20)
\end{align}</p>
</div>
<div class="paragraph">
<p>公式(5.20)就是 <code>LDA模型</code> 的 <code>Gibbs Sampling</code> 公式，这个概率其实是\(doc &#8594; topic &#8594; word\)的路径概率，由于\(topic\)有\(K\)个，所以 <code>Gibbs Sampling</code> 公式的物理意义就是在这\(K\)条路径中进行采样。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/gibbs_path.png" alt="gibbs path" width="50%" height="55%">
</div>
<div class="title">图 24. Gibbs采样路径概率</div>
</div>
</div>
<div class="sect4">
<h5 id="_模型训练和推演">5.3.3.7. 模型训练和推演</h5>
<div class="paragraph">
<p>有了 <code>LDA</code> 模型，当然我们的目标有两个<br></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>估计模型中的参数\(\vec{\varphi^{1}}, &#8230;&#8203;, \vec{\varphi^{K}}\) 和 \(\vec{\theta^{1}}, &#8230;&#8203;, \vec{\theta^{M}}\)</p>
</li>
<li>
<p>对新来的一篇文档，我们能够计算这篇文档的主题分布</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>训练的过程就是通过 <code>Gibbs Sampling</code> 获取预料中的\((z, w)\)样本，而模型中的所有参数都可以基于最终采样得到的样本进行估计，训练的流程为<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/train.png" alt="train" width="50%" height="55%">
</div>
<div class="title">图 25. LDA训练过程</div>
</div>
<div class="paragraph">
<p>由这个 <code>topic-word</code> 频率矩阵我们可以计算每一个\(p(word|topic)\)概率，从而算出模型的参数，通常在 <code>LDA</code> 模型训练的过程中，我们是取 <code>Gibbs Sampling</code> 收敛之后的\(n\)个迭代的结果进行平均来做参数估计，这样模型质量更高。<br></p>
</div>
<div class="paragraph">
<p>有了模型，对于新来的文档，我们如何做主题语义分布的计算呢？基本上推理和训练的过程完全类似。对于新的文档，我们只要认为 <code>Gibbs Sampling</code> 公式中的\(\vec{\varphi_{k,t}}\)部分是稳定不变的，是由训练预料得到的模型提供的，
所以采样的过程中我们只要估计该文档的\(topic\)分布就好了。<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/inference.png" alt="inference" width="50%" height="55%">
</div>
<div class="title">图 26. LDA推演过程</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_变分em算法求解plsa模型">5.3.4. 变分EM算法求解pLSA模型</h4>
<div class="paragraph">
<p>由于后面会有独立的章节介绍EM算法，因此本节我们介绍其基本原理，在不影响pLSA算法正常运算逻辑基础上尽可能简化介绍。<br></p>
</div>
<div class="sect4">
<h5 id="_em算法">5.3.4.1. EM算法</h5>
<div class="paragraph">
<p>EM算法(<code>Expectation Maximization algorithm</code>),又称期望最大算法，其基本思想是：首先随机选取一个值去初始化待估计的值\(\theta^{0}\)，然后不断迭代寻找更优\( \theta^{n+1}\)的使得其似然函数比原来的要大。
即通过不断迭代更新，提高似然函数的概率值，使得最大似然函数值取极值点的参数就是最优化参数。<br></p>
</div>
<div class="paragraph">
<p>所以EM算法的一般步骤为：<br></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="openblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>随机选取或者根据先验知识初始化\(\theta^{0}\);</p>
</li>
<li>
<p>不断迭代下述两步：</p>
<div class="ulist">
<ul>
<li>
<p>给出当前的参数估计\(\theta^{n}\)，计算似然函数\(L(\theta)\)的下界\(Q(\theta; \theta^{n})\);</p>
</li>
<li>
<p>重新估计参数\(\theta\)，即求\(\theta^{n+1}\),使得\( \theta^{n+1} = \underset{\theta}{argmax} \hspace{0.1cm} Q(\theta; \theta^{n})\);</p>
</li>
</ul>
</div>
</li>
<li>
<p>上述第二步后，如果\(L(\theta)\) (即\(Q(\theta; \theta^{n})\))收敛，则退出算法，否则继续回到第二步。</p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>下面给出推导公式，假设有训练集\(\left \{ x^{1}, x^{2}, &#8230;&#8203; , x^{m} \right \}\),包含\(m\)个独立样本，希望从中找到该数组的模型\(p(x,z)\)的参数。其中\(z\)是隐变量，
然后通过极大似然估计建立目标函数(对数似然函数)<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
l(\theta) &amp; = \sum_{i=1}^{m}log \hspace{0.1cm} p(x;\theta) \\
&amp; = \sum_{i=1}^{m} log\hspace{0.1cm}\sum_{z}^{}p(x;z;\theta)
\end{align}</p>
</div>
<div class="paragraph">
<p>这里\(z\)是隐变量，要找到参数的估计很难，我们的策略是建立\(l(\theta)\)的下界，并且求该下界的最大值；重复这个过程，直到收敛到局部最大值。<br></p>
</div>
<div class="paragraph">
<p>令\(Q_{i}\)为\(z\)的某一个分布，\(Q_{i} \geq 0\)，根据 <code>Jenssen</code> 不等式，有<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\sum_{i=0}^{m} log\hspace{0.1cm}p(x^{i} ; \theta) &amp; = \sum_{i=0}^{m} log\sum_{z^{0} }^{ z^{m} }p(x^{i} ; z^{i}; \theta) \\
&amp; = \sum_{i=0}^{m} log\sum_{z^{0} }^{ z^{m} } Q_{i}(z^{i}) \frac{p(x^{i} ; z^{i}; \theta)}{Q_{i}(z^{i})} \\
&amp; \geq \sum_{i=0}^{m} \sum_{z^{0} }^{ z^{m}} Q_{i}(z^{i}) log  \frac{p(x^{i} ; z^{i}; \theta)}{Q_{i}(z^{i})}
\end{align}</p>
</div>
<div class="paragraph">
<p>要理解上面不等式的转换很简单，将\(Q_{i}(z^{i})\)看作 <code>Jenssen</code> 不等式中的平衡因子，且\(\sum_{i=0}^{m} Q_{i}(z^{i})=1\)，我们要寻找的下界就是不等式取等号，
若要让 <code>Jenssen</code> 不等式，则必须让两个点重合，点的值为固定常量，即满足<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\frac{ p(x^{i} ; z^{i}; \theta)}{Q_{i}(z^{i})}=C\]
</div>
</div>
<div class="paragraph">
<p>所以可得<br>
\begin{align}
Q_{i}(z^{i}) &amp; = \frac{p(x^{i} , z^{i} ; \theta)}{ \sum_{i=0}^{m} p(x^{i} , z^{i} ; \theta) } \\
&amp; =  \frac{p(x^{i} , z^{i} ; \theta)}{ p(x^{i}  ; \theta) } \\
&amp; =  p(z^{i} \mid x^{i} ; \theta)
\end{align}</p>
</div>
<div class="paragraph">
<p>最后得导EM算法的正题框架<br></p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/em.png" alt="em" width="50%" height="55%">
</div>
<div class="title">图 27. EM算法伪代码</div>
</div>
</div>
<div class="sect4">
<h5 id="_求解plsa算法">5.3.4.2. 求解pLSA算法</h5>
<div class="paragraph">
<p>首先尝试从矩阵的角度来描述两个待估计的变量\(p(w_{j} \mid z_{k})\)和\(p(z_{k} \mid d_{i})\)。<br></p>
</div>
<div class="paragraph">
<p>假定用\(\phi_{k}\)表示词表\(V\)在主题\(z_{k}\)上一个多项分布，则\(\phi_{k}\)可以表示为一个向量，每个元素\(\phi_{k,j}\)表示词项\(w_{j}\)在主题\(z_{k}\)上的概率，即<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(w_{j} \mid z_{k})= \phi_{k,j}, \hspace{1cm} \sum_{w_{j}\in V}\phi_{k,j} = 1\]
</div>
</div>
<div class="paragraph">
<p>用\(\theta_{i}\)表示主题\(Z\)在文档\(d_{i}\)上的一个多项分布，则\(\theta_{i}\)也可以表示为一个向量，每个元素\(\theta_{i,k}\)表示主题\(z_{k}\)出现在文档\(d_{i}\)中的概率，即<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(z_{k} \mid d_{i})= \theta_{i,k}, \hspace{1cm} \sum_{z_{i}\in Z}\theta_{i,k} = 1\]
</div>
</div>
<div class="paragraph">
<p>这样巧妙的把\(p(w_{j} \mid z_{k})\)和\(p(z_{k} \mid d_{i})\)转换为了两个矩阵，换言之我们最终要求解的参数是两个矩阵<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\Phi =\left \{ \phi_{1},...,\phi_{K} \right \}, \hspace{1cm} z_{k}\in Z \\
\Theta=\left \{ \theta_{1},...,\theta_{M} \right \}, \hspace{1cm} d_{i}\in D\]
</div>
</div>
<div class="paragraph">
<p>由于每篇文章的词和词之间是相互独立的，所以整篇文章\(N\)个词的分布为<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(W \mid d_{i}) = \prod_{j=1}^{N}p(d_{i}, w_{j})^{ n(d_{i}, w_{j})}\]
</div>
</div>
<div class="paragraph">
<p>而由于文章和文章之间也是相互独立的，所以整个预料的词分布为<br></p>
</div>
<div class="stemblock">
<div class="content">
\[p(W \mid D) =\prod_{i=0}^{M} \prod_{j=1}^{N}p(d_{i}, w_{j})^{ n(d_{i}, w_{j})}\]
</div>
</div>
<div class="paragraph">
<p>其中\(n(d_{i}, w_{j})\)表示词项\(w_{j}\)在文档\(d_{i}\)中的词频，\(n(d_{i})\)表示文档\(d_{i}\)中词的总数,显然\(n(d_{i}) = \sum_{w_{j} \in V}n(d_{i}, w_{j})\)。
从而得到整个语料库的词分布的对数似然函数<br>
\begin{align}
l(\Phi,\Theta) &amp; = \sum_{i=1}^{M} \sum_{j=1}^{N}n(d_{i}, w_{j})log\hspace{0.1cm}p(d_{i}, w_{j}) \\
&amp; = \sum_{i=1}^{M}n(d_{i})(logp(d_{i}) + \sum_{j=1}^{N} \frac{n(d_{i},w_{j})}{n(d_{i})}) log\sum_{k=1}^{K}p(w_{j} \mid z_{k}) p(z_{k}, d_{i}) \\
&amp; = \sum_{i=1}^{M}n(d_{i})(logp(d_{i}) + \sum_{j=1}^{N} \frac{n(d_{i},w_{j})}{n(d_{i})}) log\sum_{k=1}^{K}\phi_{k,j}\theta_{i,k}
\end{align}</p>
</div>
<div class="paragraph">
<p>现在，我们的工作就是最大化这个对数似然函数，求解参数\(\phi_{k,j}\)和\(\theta_{i,k}\)，对于这种隐含变量的最大似然估计，可以使用EM算法，典型的EM算法分成两步，先E后M。<br></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="ulist">
<ul>
<li>
<p>E-step:假定参数已知，计算此时隐变量的后验概率，利用贝叶斯公式，可得<br>
\begin{align}
p(z_{k} \mid d_{i}, w_{j}) &amp; = \frac{p(z_{k}, d_{i}, w_{j})}{\sum_{l=1}^{K} p(z_{l}, d_{i}, w_{j})} \\
&amp; = \frac{ p(w_{j} \mid d_{i}, z_{k}) p(z_{k} \mid d_{i}) p(d_{i}) }{\sum_{l=1}^{K} p(w_{j} \mid d_{i}, z_{l}) p(z_{l} \mid d_{i}) p(d_{i})} \\
&amp; = \frac{ p(w_{j} \mid z_{k}) p(z_{k} \mid d_{i})}{\sum_{l=1}^{K} p(w_{j} \mid z_{l}) p(z_{l} \mid d_{i})} \\
&amp; = \frac{ \phi_{k,j} \theta_{i,k} }{\sum_{l=1}^{K}\phi_{l,j} \theta_{i,l}}
\end{align}</p>
</li>
<li>
<p>M-step:这样隐变量的乘积可以用后验概率表示，将其代入似然函数，最大化对数似然，求解相应参数。</p>
</li>
</ul>
</div>
</div>
</div>
<div class="paragraph">
<p>由于文档长度\(p(d_{i}) \propto n(d_{i})\)可单独计算，去掉它不影响最大似然函数，此外，根据 <code>E-step</code> 结果，将\(\phi_{k.j} \theta_{i,k}=\) \(p(z_{k} \mid d_{i}, w_{j})\sum_{l=1}^{K}\phi_{l,j}\theta_{i,l}\)带入似然函数,得到:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[l=\sum_{i=1}^{M}\sum_{j=1}^{N}n(d_{i}, w_{j})\sum_{k=1}^{K}p(z_{k} \mid d_{i}, w_{j}) \hspace{0.1cm} log( \phi_{k,j}, \theta_{i,k} )  \hspace{1cm}(5.21)\]
</div>
</div>
<div class="paragraph">
<p>这是一个多元函数求极值问题，并且有如下的约束条件:<br></p>
</div>
<div class="stemblock">
<div class="content">
\[\sum_{j=1}^{N}\phi_{k,j} = 1 \\
\sum_{k=1}^{K}\theta_{i,k} = 1\]
</div>
</div>
<div class="paragraph">
<p>一般这种带约束条件的极值问题，常用的方法便是拉格朗日乘法，即通过引入拉格朗日乘子将目标函数和约束条件融合到一起，转化为无约束条件的优化问题。<br></p>
</div>
<div class="stemblock">
<div class="content">
\[H = L^{c} + \sum_{k=1}^{K}\gamma_{k}(1-\sum_{j=1}^{N} \phi_{k,j})  + \sum_{i=1}^{M}\rho_{i}(1-\sum_{k=1}^{K}\theta_{i,k})\]
</div>
</div>
<div class="paragraph">
<p>分别对\(\phi_{k,j}\)和\(\theta_{i,k}\)求导，令导数为0，得到<br></p>
</div>
<div class="paragraph">
<p>\begin{align}
\phi_{k,j} &amp; = \frac{ \sum_{i=1}^{M} n(d_{i}, w_{j}) p(z_{k} \mid d_{i}, w_{j}) }{\sum_{j=1}^{M} \sum_{i=1}^{N} n(d_{i}, w_{j}) p(z_{k} \mid d_{i}, w_{j})} \\
\theta_{i,k} &amp; = \frac{ \sum_{j=1}^{N} n(d_{i}, w_{j}) p(z_{k} \mid d_{i}, w_{j}) }{n(d_{i})}
\end{align}</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_终篇">5.4. 终篇</h3>
<div class="paragraph">
<p>走到这里的人，我相信你们一定被 <code>LDA</code> 模型深深的吸引，这也是我为何要花费大量时间编写这个章节的原因，LDA原始论文中是基于变分 <code>EM</code> 算法进行参数公式推导，当然对应的模型实现也可以采用变分法，本章我们对基于 <code>Gibbs Sampling</code> 的 <code>LDA</code> 模型进行了详细描述，整章的编写花费了大概两天的时间，是在太累了，
后续有时间再将变分法补上(已经补上)。</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_最大熵理论和多分类器">6. 最大熵理论和多分类器</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_em_算法">7. <code>EM</code> 算法</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_决策树和集成学习">8. 决策树和集成学习</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_推荐系统">9. 推荐系统</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_聚类和近邻算法">10. 聚类和近邻算法</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_神经网络">11. 神经网络</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_隐马尔可夫模型">12. 隐马尔可夫模型</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_条件随机场">13. 条件随机场</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_深度学习概论">14. 深度学习概论</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_深度学习中的优化算法">15. 深度学习中的优化算法</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_深度学习中的正则化">16. 深度学习中的正则化</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_深度神经网络">17. 深度神经网络</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_卷积神经网络">18. 卷积神经网络</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_循环神经网络">19. 循环神经网络</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_神经网络番外篇">20. 神经网络番外篇</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_生成对抗网络">21. 生成对抗网络</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_迁移学习">22. 迁移学习</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_数学之美">23. 数学之美</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_深度学习框架">24. 深度学习框架</h2>
<div class="sectionbody">

</div>
</div>
<div class="sect1">
<h2 id="_走进互联网">25. 走进互联网</h2>
<div class="sectionbody">

</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
最后更新时间 2018-07-18 12:01:57 CST
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>